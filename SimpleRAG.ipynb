{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFBwAWZDfZHrRgv5FErJzw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Koks-creator/SimpleRAG/blob/main/SimpleRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode langchain==0.2.12 langchain_community==0.2.11 openai==1.39.0 pypdf sentence-transformers chromadb unstructured python-pptx python-magic nltk==3.9.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rWGiHMF265EE",
        "outputId": "e2418e4a-cfe8-4fee-dd36-660c63b87db6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n",
            "Requirement already satisfied: langchain==0.2.12 in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
            "Requirement already satisfied: langchain_community==0.2.11 in /usr/local/lib/python3.10/dist-packages (0.2.11)\n",
            "Requirement already satisfied: openai==1.39.0 in /usr/local/lib/python3.10/dist-packages (1.39.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
            "Requirement already satisfied: unstructured in /usr/local/lib/python3.10/dist-packages (0.15.7)\n",
            "Requirement already satisfied: python-pptx in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (0.4.27)\n",
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.12) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.12) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.12) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.12) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.12) (0.2.34)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.12) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.12) (0.1.104)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.12) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.12) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.12) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.12) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.2.11) (0.6.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.39.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.39.0) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.39.0) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.39.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.39.0) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.39.0) (4.12.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1) (2024.5.15)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.112.1)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.6)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.5.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.19.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.3)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.2.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.4)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (30.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.7)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.12.1)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.0.9)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.9.6)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.25.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.16.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.9.5)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (3.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.39.0) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.39.0) (1.2.2)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.2.11) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.2.11) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.38.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.39.0) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.39.0) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.39.0) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain==0.2.12) (1.33)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.26.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (71.0.4)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.2.12) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.2.12) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.2.12) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.12) (3.0.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.20)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.23.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (7.0.1)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.6)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client->unstructured) (4.1.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain==0.2.12) (3.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you wanna use ppt/pptx files - run 2nd and 3rd cells - if not, you can skip them since it takes few minuts to install**"
      ],
      "metadata": {
        "id": "ev2RKrHqAkCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install libreoffice"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SJZw53Qg4mNf",
        "outputId": "405c5557-0dbf-493e-eeb5-d37400ac286a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libreoffice is already the newest version (1:7.3.7-0ubuntu0.22.04.6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqiz8xgQ5CDq",
        "outputId": "44866cb6-d78e-405a-bf16-86a2efeaa850"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "2iXGpzBL6pn_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from typing import Optional\n",
        "from dataclasses import dataclass, field\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
        "from openai import OpenAI\n",
        "from unidecode import unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Create Vector Database**"
      ],
      "metadata": {
        "id": "VGg-8Eu0QZi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHUNK_SIZE = 20000,\n",
        "CHUNK_OVERLAP = 20\n",
        "DB_PATH = \"chroma_db_ncnn\"\n",
        "OVERWRITE_DB = True\n",
        "K=5"
      ],
      "metadata": {
        "id": "vxldip-pQLi7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataLoader():\n",
        "    _loaders: dict = field(init=False, repr=False)\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        self._loaders = {\n",
        "            \"pdf\": PyPDFLoader,\n",
        "            \"txt\": TextLoader,\n",
        "            \"md\": TextLoader,\n",
        "            \"ppt\": UnstructuredPowerPointLoader\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_html_tags(text: str) -> str:\n",
        "        clean = re.compile('<.*?>')\n",
        "        return re.sub(clean, '', text)\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        return self.remove_html_tags(unidecode(text))\n",
        "\n",
        "    def load_file(self, file_path: str) -> list:\n",
        "        file_name, file_extension = os.path.splitext(file_path)\n",
        "        try:\n",
        "            file_extension = file_extension.replace(\".\", \"\")\n",
        "\n",
        "            loader = self._loaders[file_extension](file_path)\n",
        "            data = loader.load()\n",
        "            for d in data:\n",
        "                d.page_content = self.clean_text(text=d.page_content)\n",
        "\n",
        "            return data\n",
        "        except KeyError:\n",
        "            return []\n",
        "\n",
        "    def load_files(self, data_folder: str) -> list:\n",
        "        docs = []\n",
        "        files = os.listdir(data_folder)\n",
        "        for file in files:\n",
        "              docs.extend(self.load_file(file_path=f\"{data_folder}/{file}\"))\n",
        "        return docs\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class VectorDatabase:\n",
        "    db_path: str\n",
        "    chunk_size: int\n",
        "    chunk_overlap: int\n",
        "    separators: list = field(default_factory=lambda: [\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \",\", \" \"])\n",
        "    model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    device: str = \"cpu\"\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        self.embedding_func = HuggingFaceEmbeddings(model_name=self.model_name, model_kwargs={\"device\": self.device})\n",
        "\n",
        "    def create_db(self, data: list, overwrite: bool = True) -> Chroma:\n",
        "        if overwrite:\n",
        "            self.delete_collections()\n",
        "        vectorstore = Chroma.from_documents(data, self.embedding_func, persist_directory=self.db_path)\n",
        "\n",
        "        return vectorstore\n",
        "\n",
        "    def get_db(self) -> Chroma:\n",
        "        return Chroma(persist_directory=self.db_path, embedding_function=self.embedding_func)\n",
        "\n",
        "    def get_db_data(self) -> dict:\n",
        "        return self.get_db().get()\n",
        "\n",
        "    def query_db(self, query: str, k: int = 5) -> list:\n",
        "        vector_db = self.get_db()\n",
        "        results = vector_db.similarity_search(query, k=k)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def delete_collections(self) -> None:\n",
        "        ids = self.get_db_data()[\"ids\"]\n",
        "        if ids:\n",
        "            self.get_db().delete(ids)\n",
        "        else:\n",
        "            print(\"Nothing to delete\")\n"
      ],
      "metadata": {
        "id": "pt_95vMyBQJB"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl = DataLoader()\n",
        "\n",
        "res = dl.load_files(\"/content/data\")"
      ],
      "metadata": {
        "id": "LxL-IziwC4aI"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db = VectorDatabase(\n",
        "    db_path=DB_PATH,\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP,\n",
        "    )\n",
        "\n",
        "vector_db.create_db(data=res, overwrite=OVERWRITE_DB)"
      ],
      "metadata": {
        "id": "KUMiK7XfVpPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "fa2e5fac-525c-4d44-e662-49dfffc1bda0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.chroma.Chroma at 0x7cf7578b9b40>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db.get_db_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtWHrUt96gfi",
        "outputId": "73d765aa-a832-4a7d-cf7f-7c5cb69a77a0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': ['0092db99-53b4-472d-9eec-01d60b2ec1a7',\n",
              "  '02fed92b-c377-442b-9338-d8cb29f6d0b6',\n",
              "  '04b67caf-439d-4b46-a77a-8995f5115f7f',\n",
              "  '06210e41-b995-4e66-98c3-a7c2d489e437',\n",
              "  '06d33b10-1d27-4553-8ff8-06464e62f19d',\n",
              "  '07a4459f-f2d7-49b9-b700-d8d4629d02c1',\n",
              "  '0c2a2433-22f4-4d36-90d6-7dba959ca06a',\n",
              "  '114eb477-7666-41bf-8d83-868b338e30ab',\n",
              "  '143fbf55-a151-4458-888f-f45507e2dddb',\n",
              "  '1c8f4345-0a40-42a6-8406-33ee3996e988',\n",
              "  '22552521-8c85-495f-a446-faa9128f8657',\n",
              "  '2445de51-72ca-44ec-967e-19586288241b',\n",
              "  '27684619-b824-4c48-99fc-08a485b521ac',\n",
              "  '2787dd6e-99fc-44c0-a9ee-8ce91a753eb8',\n",
              "  '2d89f62a-4bb3-430b-b711-cd2afe67e6d5',\n",
              "  '3aa82eb5-ce48-49ee-bd8d-aebb65aad588',\n",
              "  '3c7567d0-0792-484c-bc59-18faee0cdda9',\n",
              "  '3fc7db6a-0ce8-49e0-9255-09c7c2dc70fc',\n",
              "  '45a3e06c-cb4d-4048-8295-be2c8a3c784a',\n",
              "  '46cc2877-f784-4d28-81dd-87da5e5281f5',\n",
              "  '49ba51e5-d6cb-4b23-9657-848bdb1a3cdf',\n",
              "  '4dc83a0c-65ce-4f2b-8bcc-3342abfe4404',\n",
              "  '4eb83cae-4003-4624-b01a-3168818c3060',\n",
              "  '4f2fe6b7-ddcb-4ecd-9c1d-f7d1d5ea846e',\n",
              "  '534f8957-6aa6-42bc-b79c-fdba9412a7d9',\n",
              "  '53f311dc-88ec-4636-b493-b995b69d48a6',\n",
              "  '54a57908-4edc-4c92-bc6c-e741172fc10e',\n",
              "  '5a21c96b-12a3-49b4-b377-0fa1880ccbb6',\n",
              "  '60159b13-8a36-4fa8-ab4e-016ef9ebc4c8',\n",
              "  '634044bb-cd33-46c9-af96-31c9d33880e9',\n",
              "  '68943e92-2128-4037-86b0-39130a983c77',\n",
              "  '71f5f21a-0806-492d-bcfb-76e4e8e64a22',\n",
              "  '73b1184a-1053-4dca-a660-fc373e180d9d',\n",
              "  '772508df-f063-4729-9c71-541e89d820cc',\n",
              "  '7a16d0a2-2353-4c3f-8f91-ff42db3d78b5',\n",
              "  '7f2da4bc-7a62-4c55-bd0f-ed56ec659276',\n",
              "  '7f8ec0d6-0761-4f7d-ad50-4d13febe4ad9',\n",
              "  '84f2b4a1-8cea-42e6-87ca-5361fbdcd07c',\n",
              "  '852a77b6-d253-45d6-98e9-2816661a4918',\n",
              "  '8601d36f-d5fe-40f6-8e66-46524ecc2b46',\n",
              "  '89e7ac5d-cd8f-4ead-9b4f-11ac9ef59f33',\n",
              "  '8daca0bf-02e4-4ce8-ae7c-f982ecb5983f',\n",
              "  '935f9743-2716-4a4e-8a2b-a21082d6a2d2',\n",
              "  '95585ce6-ed55-4dbb-ba77-81110bd77d0e',\n",
              "  '95d01527-8afa-40bf-99e5-65d594432161',\n",
              "  '9766c6f9-8934-4c68-8d0c-edd7d68e5cf2',\n",
              "  '9b3f70a5-55b0-4550-9a3e-58faa2a1b733',\n",
              "  '9f021508-660b-4a1a-8897-f05c4a6402a5',\n",
              "  'a21441d3-beb0-48e2-b797-56376b1bea86',\n",
              "  'a26c516f-4fb9-445b-a02b-8c799b4e73c6',\n",
              "  'a5ef8ef4-822c-4012-96c0-d1c7287f1570',\n",
              "  'a78b6a31-adcf-4ebe-81cc-c91c3e8f666b',\n",
              "  'a8012b6a-6393-4d50-a1ab-47af944b519b',\n",
              "  'ac09b35e-341c-427e-b6e8-386d4065b9ab',\n",
              "  'adb6a2c2-e33c-4ee9-9431-0d38353177f0',\n",
              "  'ae448b7a-c33a-498e-908f-0e68b6738426',\n",
              "  'af54a2db-ac6d-41e3-b6f7-ba69b51f4315',\n",
              "  'afd78fe6-87f1-4052-82bd-1131f1ffd3e7',\n",
              "  'b1de43b7-1f47-4bd1-865a-90b60da7dbc3',\n",
              "  'b3d0bfb5-fe78-4232-995c-88105c6fd064',\n",
              "  'b3da113f-7031-44fc-a448-e8c4bd6bc14b',\n",
              "  'b40f5c85-6999-4921-8ff7-4706d5df4b91',\n",
              "  'b4694d5b-52ad-4bbd-b01e-b8c598b635f8',\n",
              "  'b80cbb47-0192-47e4-9234-884700a4fa90',\n",
              "  'bcc5235e-c98f-4545-b5bd-73b4c4667ad8',\n",
              "  'bd944dac-4b45-4dbf-82fd-7d1533836039',\n",
              "  'bde5c627-dfe3-4f50-aaeb-b161937daaad',\n",
              "  'bf86e117-022f-40d1-a060-5e967578a2a0',\n",
              "  'c2878884-8b2e-4aaa-8c02-284dfe6a0c74',\n",
              "  'c6bc6ded-8295-4a28-a2c0-2ea2596599fc',\n",
              "  'c9be46f2-6ebf-4a3a-b4a8-435d4d901e15',\n",
              "  'cd2c1664-7127-4a05-af8f-c8d7c50f79bb',\n",
              "  'd05443f1-c7aa-41a9-9184-6d3ac6a7246d',\n",
              "  'd1a89173-8737-49b9-acf6-6a2813926afd',\n",
              "  'd7b7dfa5-164d-4143-9dad-d408208d537d',\n",
              "  'ded1d299-2ab1-4b27-afaf-f4712910153c',\n",
              "  'df5af7c7-9eac-4659-8dc7-d7018d91cc92',\n",
              "  'e3189433-4286-4346-aaad-2a6e5d7f410c',\n",
              "  'e8bba0d7-adfe-4f80-b45d-bf9290b5702f',\n",
              "  'e910b577-3382-4ebd-92c2-da4ad84c5a88',\n",
              "  'e93edc5b-e664-40c0-9ce5-62a2cf045787',\n",
              "  'eb58a4f8-e974-4a74-b96e-5d64685e51b9',\n",
              "  'ed3f1589-4719-40fc-9c54-a10ad2758f84',\n",
              "  'ee51cc35-c6f5-4c32-bbc0-9051fb6f3530',\n",
              "  'ef4009c2-ffad-40b4-9e68-f88721e65db1',\n",
              "  'f121129a-7aff-4d9e-8a9d-0f46fb2ff8d6',\n",
              "  'f3969d18-8ad7-4fe4-8d98-e4b31698e911',\n",
              "  'f45fd1f0-e4cd-42f0-8cea-9a23d5938452',\n",
              "  'fc8039e3-1709-4038-a9fd-99f3f2a54e1a'],\n",
              " 'embeddings': None,\n",
              " 'metadatas': [{'page': 7, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 18, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 45, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 32, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 37, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 26, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 34, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 7, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 20, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 3, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 12, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 25, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 17, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 11, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 31, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 37, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 30, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 16, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 17, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 33, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'source': '/content/data/README.md'},\n",
              "  {'page': 2, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 4, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 0, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 10, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 44, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 2, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 21, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 35, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 15, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 19, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 19, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 14, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 6, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 10, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 29, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 36, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 23, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 0, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 38, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 8, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 22, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 5, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 39, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 16, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 9, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 26, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 13, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 29, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 5, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 12, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'source': '/content/data/randomData.txt'},\n",
              "  {'page': 34, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 24, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 23, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 43, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 36, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 39, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 28, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 31, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 15, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 6, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 24, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 1, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 18, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 1, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 20, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 27, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 13, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 8, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 27, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 41, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 46, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 28, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 32, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 14, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 40, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 9, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 35, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 33, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 38, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 11, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 3, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 22, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 42, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 21, 'source': '/content/data/CNN.pdf'},\n",
              "  {'page': 30, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 25, 'source': '/content/data/CI-RNN-LSTM.pdf'},\n",
              "  {'page': 4, 'source': '/content/data/CI-RNN-LSTM.pdf'}],\n",
              " 'documents': ['We will use \\n recurrent neural networks \\n to overcome the presented difficulties and to allow \\nthe network to share features and weights and use the context of previous sequence elements:\\nooyt=gyWyaat+by\\ngy\\nis usually \\n sigmoid\\nat=gaWaaat-1+Waxxt+ba\\nga\\nis usually \\n ReLU\\n or tanh\\nIn the above network, we put the subsequent elements (e.g. words) on the inputs of \\nthe subnetworks which share weights with the other subnetworks (\\n in a \\n nutshell, all these \\nsubnetworks are the same network), so the position of the element (word) in the sequence\\ncan be different without harm in the representation of this word by the neural network.\\nThanks to the connections to the next subnetwork, we can use the context of the processed, \\nprevious elements (words) represented by the outputs of previous subnetworks at\\n.\\nBefore we\\n start,\\n we\\nshould\\n introduce\\n what\\n sequences\\n or\\nsequential\\n data\\n are\\nin\\ndetail.\\nWe will use recurrent neural networks\\n',\n",
              "  'Additional Architectural Features of RNN\\nWe can use the output to convey contextual information of the next state:\\n',\n",
              "  'Deep Grid and Convolutional \\n LSTM\\n Networks\\nStacked LSTM (sLSTM)\\n Convolutional LSTM (cLSTM)\\nGrid LSTM where cells are\\nconnected between\\nnetwork \\n layers \\n as well as\\nalong \\n the \\nspatiotemporal\\ndimensions of the data:\\n',\n",
              "  'Unfortunately, there is no harmonized format for such a descrip-\\ntion. An example is provided in Fig. 19.108 Maria Vakalopoulou et al.\\nFig. 19 A drawing describing a CNN architecture. Classically, it is composed of two main parts. Here \\n16@3x3x3 means that 16 features with a 3x3x3 convolution kernel will be computed. For the pooling \\noperation, the kernel size is also mentioned (2x2). Finally, the stride is systematically indicated \\nOne of the first CNN architectures that follow this paradigm is \\nthe AlexNet architecture [ 54]. AlexNet was one of the first papers \\nthat empirically indicated that the ReLU activation function makes \\nthe convergence of CNNs faster compared to other non-linearities \\nsuch as the tanh. Moreover, it was the first architecture that achieved a top 5 error rate of 18.2% on the ImageNet dataset, \\noutperforming all the other methods on this benchmark by a huge margin (about 10%). Prior to AlexNet, best-performing \\nmethods were using (very sophisticated) pre-extracted features and classical machine learning. After this advance, deep learning in general and CNNs, in particular, became very active research \\ndirections to address different computer vision problems. This resulted in the introduction of a variety of architectures such as \\nVGG16 [\\n55] that reported a 7.3% error rate on ImageNet, intro-\\nducing some changes such as the use of smaller kernel filters. \\nFollowing these advances, and even if there were a lot of different \\narchitectures proposed during that period, one could mention the \\nInception architecture [ 56], which was one of the deepest archi-\\ntectures of that period and which further reduced the error rate on ImageNet to 6.7%. One of the main characteristics of this architec-\\nture was the inception modules, which applied multiple kernel \\nfilters of different sizes at each level of the architecture. To solve the problem of vanishing gradients, the authors introduced auxil-\\niary classifiers connected to intermediate layers, expecting to encourage discrimination in the lower stages in the classifier, \\nincreasing the gradient signal that gets propagated back, and \\nproviding additional regularization. During inference, these classi-fiers were completely discarded. \\nIn the following section, some other recent and commonly \\nused CNN architectures, especially for medical applications, will \\nbe presented.',\n",
              "  'Long Short\\n -\\nTerm Memory (LSTM)\\nIn the next step, the LSTM decides what new information will be stored in the \\ncell state: First, a \\n sigmoid layer \\n \\ncalled the \\n input gate layer \\n decides which values \\nwe shall update. Next, a \\n tanh\\n layer\\n creates a vector of new candidate values, C\\nt\\n, \\nthat could be added to the state. In the next step, we shall combine these two to \\ncreate an update to the state.\\nThe \\n input gate (\\n i\\n) \\nof a simple LSTM decides about \\n the addition \\n of new stuff from \\nthe present input to our present cell state scaled by how much we wish to add \\nthem.\\nThe sigmoid layer \\n \\ndecides which values to be updated and \\n tanh\\n layer creates \\na vector for new candidates to added to \\n the present \\n cell state.\\n~',\n",
              "  'K=k00k01k02 0\\n0k 00k01k\\n0000\\n0000\\nI=i 00i01i02i 1/2102 Maria Vakalopoulou et al.\\nK = k00 k01 k02 \\nk10 k11 k12 \\nk20 k21 k22 and I = i00 i01 i02 i03 \\ni10 i11 i12 i13 \\ni20 i21 i22 i23 \\ni30 i31 i32 i33 : \\nThen, the convolution operation can be computed as a matrix \\nmultiplication between the Toepliz transformed kernel: \\nk10 k11 k12 0 k 20 k21 k22 00000 \\n02 0 k 10 k11 k12 0 k 20 k21 k22 0000 \\nk00 k01 k02 0 k 10 k11 k12 0 k 20 k21 k22 0 \\n0 k 00 k01 k02 0 k 10 k11 k12 0 k 20 k21 k22 \\nand a reshaped input: \\n03 i10 i11 i12 i13 i20 i21 i22 i23 i30 i31 i32 i33/C138 : \\nThe produced output will need to be reshaped as a 2x2 matrix \\nin order to retrieve the convolution output. This matrix multiplica-\\ntion implementation is quite illuminating on a few of the most \\nimportant properties of the convolution operation. These proper-\\nties are the main motivation behind using such elements in deep \\nneural networks. \\nBy transformi ng the convolution operation to a matrix multi-\\nplication operation, it is evident that it can fit in the formalization of the linear functions, which has already been presented in Subhead-\\ning \\n2.3. As such, deep neural networks can be designed in a way to \\nutilize trainable convolution kernels. In practice, multiple convolu-\\ntion kernels are learned at each convolutional block, while several of \\nthese trainable convolutional blocks are stacked on top of each \\nother forming deep CNNs. Typically, the output of a convolution \\noperation is called a feature map or just features. \\nAnother important aspect of the convolution operation is that \\nit requires much fewer parameters than the fully connected \\nMLP-based deep neural networks. As it can also be seen from the \\nK matrix, the exact same parameters are shared across all locations. \\nEventually, rather than learning a different set of parameters for the \\ndifferent locations of the input, only one set is learned. This is \\nreferred to as parameter sharing or weight sharing and can greatly \\ndecrease the amount of memory that is required to store the \\nnetwork parameters. An illustration of the process of weight sharing \\nacross locations, together with the fact that multiple filters (result-\\ning in multiple feature maps) are computed for a given layer, is illustrated in Fig. \\n13. The multiple feature maps for a given layer are \\nstored using another dimension (see Fig. 14), thus resulting in a 3D',\n",
              "  'Gates \\n of LSTMs\\nGates\\n are a way to optionally let information through.\\nThey are composed out of a sigmoid neural net layer and \\na pointwise multiplication operation.\\nThe sigmoid layer outputs numbers between zero and one, \\ndescribing how much of each component should be let \\nthrough. A value of \\n zero\\n means \"\\n let nothing through\\n ,\" while \\na value of \\n one\\n means \"\\n let everything through!\\n \"\\nAn LSTM has three of \\nthese gates, to protect \\nand control the cell state.',\n",
              "  'width of each k layer (the number of neurons d k). Overall, there are \\nno rules for the choice of the K and d k parameters that define the \\narchitecture of the MLP. However, it has been shown empirically \\nthat deeper models perform better. In Fig. 4, an overview of \\n2 different networks with 3 and 11 hidden layers is presented \\nwith respect to the number of parameters and their accuracy. For \\neach architecture, the number of parameters varies by changing the \\nnumber of neurons d k. One can observe that, empirically, deeper \\nnetworks achieve better performance using approximately the same \\nor a lower number of parameters. Additional evidence to support \\nthese empirical findings is a very active field of research [ 22, 23]. Deep Learning: Basics and CNN 83\\nx1 \\nx2 \\nxp zk ,3 zk ,2 zk ,1 \\nzk, d k X X X X 1 ^y \\n2 ^y \\nFig. 3 An example of a deep neural network. The input layer, the kth layer of the deep neural network, and the \\noutput layer are presented in the figure \\nFig. 4 Comparison of two different networks with almost the same number of parameters, but different \\ndepths. Figure inspired by Goodfellow et al. [ 24] \\nNeural networks can come in a variety of models and architec-\\ntures. The choice of the proper architecture and type of neural \\nnetwork depends on the type of application and the type of data.',\n",
              "  'covariate shift which changes the distribution of the inputs of each \\nlayer affecting the learning rate of the network. Even if the method \\nis quite popular, its necessity and use for the training have recently \\nbeen questioned [ 43]. 96 Maria Vakalopoulou et al.\\n3.4 State-of-the-Art \\nOptimizers Over the years, different optimizers have been proposed and widely \\nused, aiming to provide improvements over the classical stochastic \\ngradient descent. These algorithms are motivated by challenges \\nthat need to be addressed with stochastic gradient descent and are \\nfocusing on the choice of the proper learning rate, its dynamic \\nchange during training, as well as the fact that it is the same for all \\nthe parameter updates [ 44]. Moreover, a proper choice of opti-\\nmizer could speed up the convergence to the optimal solution. In \\nthis subsection, we will discuss some of the most commonly used \\noptimizers nowadays. \\n3.4.1 Stochastic Gradient \\nDescent with Momentum One of the limitations of the stochastic gradient descent is that \\nsince the direction of the gradient that we are taking is random, it \\ncan heavily oscillate, making the training slower and even getting \\nstuck in a saddle point. To deal with this problem, stochastic \\ngradient descent with momentum [ 45, 46] keeps a history of the \\nprevious gradients, and it updates the weights taking into account \\nthe previous updates. More formally: \\ngt -rgt-1 thd1-rThGdWtTh \\nDWt --etgt \\nWtth1 -Wt thDWt ,d21Th \\nwhere gt is the direction of the update of the weights in time-step \\nt and r[0, 1] is a hyperparameter that controls the contribution \\nof the previous gradients and current gradient in the current update. When r=0, it is the same as the classical stochastic gradient \\ndescent. A large value of r will mean that the update is strongly \\ninfluenced by the previous updates. \\nThe momentum algorithm accumulates an exponentially \\ndecaying moving average of the past gradients and continues to \\nmove in their direction [ 24]. Momentum increases the speed of \\nconvergence, while it is also helpful to not get stuck in places where \\nthe search space is flat (saddle points with zero gradient), since the \\nmomentum will pursue the search in the same direction as before \\nthe flat region. \\n3.4.2 AdaGrad To facilitate and speed up, even more, the training process, optimi-\\nzers with adaptive learning rates per parameter have been proposed. The adaptive gradient (AdaGrad) optimizer [\\n47] is one of them. It \\nupdates each individual parameter proportionally to their compo-\\nnent (and momentum) in the gradient. More formally:',\n",
              "  \"Deep Learning: Basics and CNN 79\\n2 Deep Feedforward Networks \\nIn this section, we will present the early deep learning approaches \\ntogether with the main functions that are commonly used in deep \\nfeedforward networks. Deep feedforward networks are a set of \\nparametric, non-linear, and hierarchical representation models \\nthat are optimized with stochastic gradient descent. In this defini-\\ntion, the term parametric holds due to the parameters that we need \\nto learn during the training of these models, the non-linearity due \\nto the non-linear functions that they are composed of, and the \\nhierarchical representation due to the fact that the output of one \\nfunction is used as the input of the next in a hierarchical way. \\n2.1 Perceptrons The perceptron [ 1] was originally developed for supervised binary \\nclassification problems, and it was inspired by works from neuros-\\ncientists such as Donald Hebb [ 17]. It was built around a \\nnon-linear neuron, namely, the McCulloch-Pitts model of a neu-\\nron. More formally, we are looking for a function f(x;w, b) such that \\nfd:;w,bTh : xp -fth1,-1g where w and b are the parameters \\nof f and the vector x=[x 1, ..., x p] is the input. The training set is \\n{(x(i) , y(i) )}. In particular, the perceptron relies on a linear model for \\nperforming the classification: \\nfdx;w,bTh= th1i f wx thb >=0\\n-1 otherwise : d1Th \\nSuch a model can be interpreted geometrically as a hyperplane \\nthat can appropriately divide data points that are linearly separable. \\nMoreover, one can observe that, in the previous definition, a per-\\nceptron is a combination of a weighted summation between the \\nelements of the input vector x combined with a step function that \\nperforms the decision for the classification. Without loss of gener-ality, this step function can be replaced by other activation functions \\nsuch as the sigmoid, hyperbolic tangent, or softmax functions (see \\nSubheading \\n2.3); the output simply needs to be thresholded to \\nassign the + 1 or-1 class. Graphically, a perceptron is presented in \\nFig. 1 on which each of the elements of the input is described as a \\nneuron and all the elements are combined by weighting with the \\nmodels' parameters and then passed to an activation function for the final decision. \\nDuring the training process and similarly to the other machine \\nlearning algorithms, we need to find the optimal parameters w and \\nb for the perceptron model. One of the main innovations of Rosen-\\nblatt was the proposition of the learning algorithm using an itera-\\ntive process. First, the weights are initialized randomly, and then \\nusing one sample (x(i) , y(i) ) of the training set, the prediction of the\",\n",
              "  '88 Maria Vakalopoulou et al.\\nIn neural networks, the loss function can be virtually any func-\\ntion that is differentiable. Below we present the two most common \\nlosses, which are, respectively, used for classification or regression \\nproblems. However, specific losses exist for other tasks, such as \\nsegmentation, which are covered in the corresponding chapters. \\nCross-Entropy Loss One of the most basic loss functions for classification problems corresponds to the cross-entropy between the expected values and \\nthe predicted ones. It leads to the following cost function: \\nJdWTh=-n \\ni =1 log P y=ydiThjx=xdiTh;W , d11Th \\nwhere P y=ydiThjx=xdiTh;W is the probability that a given sample is \\ncorrectly classified. \\nThe cross-entropy can also be seen here as the negative \\nlog-likelihood of the training set given the predictions of the net-\\nwork. In other words, minimizing this loss function corresponds to \\nmaximizing the likelihood: \\nJdWTh=  n \\ni =1 P y=ydiThjx=xdiTh;W : d12Th \\nMean Squared Error Loss For regression problems, the mean squared error is one of the most basic cost functions, measuring the average of the squares of the \\nerrors, which is the average squared difference between the pre-\\ndicted values and the real ones. The mean squared error is defined as: \\nJdWTh= n \\ni =1 jj ydiTh-fdxdiTh;WThj j2 : d13Th \\n3 Optimization of Deep Neural Networks \\nOptimization is one of the most important components of \\nneural networks, and it focuses on finding the parameters W that \\nminimize the loss function J(W). Overall, optimization is a difficult task. Traditionally, the optimization process is performed by care-\\nfully designing the loss function and integrating its constraints to \\nensure that the optimization process is convex (and thus, one can \\nbe sure to find the global minimum). However, neural networks are \\nnon-convex models, making their optimization challenging, and, in \\ngeneral, one does not find the global minimum but only a local one. \\nIn the next sections, the main components of their optimization \\nwill be presented, giving a general overview of the optimization \\nprocess, its challenges, and common practices.',\n",
              "  \"extension to the 3D case, which is often encountered in medical \\nimaging, is straightforward. Deep Learning: Basics and CNN 101\\n10 - 1 \\n10 - 1 \\n10 - 1 111 \\n000\\n-1 -1 -1 \\nOriginal image Vertical edge dete ction Horizontal edge detection \\nFig. 11 Two examples of convolutions applied to an image. One of the filters acts as a vertical edge detector \\nand the other one as a horizontal edge detector. Of course, in CNNs, the filters are learned, not predefined, so \\nthere is no guarantee that, among the learned filters, there will be a vertical/horizontal case detector, although \\nit will often be the case in practice, especially for the first layers of the architecture \\nFig. 12 Example of application of a non-linear activation function (here a ReLU) to an image \\n4.2 P roperties of the \\nConvolution Operation In the case of a discrete domain, the convolution operation can be \\nperformed using a simple matrix multiplication without the need of \\nshifting one signal over the other one. This can be essentially \\nachieved by utilizing the Toeplitz matrix transformation. The Toe-\\nplitz transformation creates a sparse matrix with repeated elements \\nwhich, when multiplied with the input signal, produces the convo-\\nlution result. To illustrate how the convolution operation can be implemented as a matrix multiplication, let's take the example of a \\n3x3 kernel (K) and a 4x4 input (I):\",\n",
              "  'beginning, has not been used in any way during training, and is \\nonly used to report the performance (see Chap. 20 for details). In \\ncase one cannot have an additional independent test set due to a \\nlack of data, one should be aware that the performance may be biased and that this is a limitation of the specific study. Deep Learning: Basics and CNN 93\\nTo avoid overfitting and improve the generalization perfor-\\nmance of the model, usually, the validation set is used to monitor \\nthe loss during the training of the networks. Tracking the training \\nand validation losses over the number of epochs is essential and provides important insights into the training process and the selected hyperparameters (e.g., choice of learning rate). Recent \\nvisualization tools such as TensorBoard\\n3 or Weights & Biases4 \\nmake this tracking easy. In the following, we will also mention \\nsome of the most commonly applied optimization techniques that \\nhelp with preventing overfitting. \\nEarly Stopping Using the reported training and validation errors, \\nthe best model in terms of performance and generalization power is \\nselected. In particular, early stopping, which corresponds to select-\\ning a model corresponding to an earlier time point than the final epoch, is a common way to prevent overfitting [\\n36]. Early stopping \\nis a form of regularization for models that are trained with an \\niterative method, such as gradient descent and its variants. Early \\nstopping can be implemented with different criteria. However, generally, it requires the monitoring of the performance of the \\nmodel on a validation set, and the model is selected when its performance degrades or its loss increases. Overall, early stopping should be used almost universally for the training of neural net-\\nworks [\\n24]. The concept of early stopping is illustrated in Fig. 8. \\nWeight Regularization Similar to other machine learning meth-\\nods (Chap. 2), weight regularization is also a very commonly used \\ntechnique for avoiding overfitting in neural networks. More specif-\\nically, during the training of the model, the weights of the network \\nstart growing in size in order to specialize the model to the training \\ndata. However, large weights tend to cause sharp transitions in the \\ndifferent layers of the network and, that way, large changes in the \\noutput for only small changes in the inputs [ 37]. To handle this \\nproblem, during the training process, the weights can be updated in \\nsuch a way that they are encouraged to be small, by adding a penalty \\nto the loss function, for instance, the l 2 norm of the parameters \\nlkWk2 , where l is a trade-off parameter between the loss and the \\nregularization. Since weight regularization is quite popular in\\n3 https:/ /www.tensorflow.org/tensorboard . \\n4 https:/ /wandb.ai/site .',\n",
              "  'nDeep Learning: Basics and CNN 87\\ng =x/C1sdbxTh \\ng \\nx =bgdxThth sdbxThd1-bgdxThTh ,d8Th \\nwhere s is the sigmoid function and b is either a constant or a \\ntrainable parameter. Swish tends to work better than ReLU on \\ndeeper models, as it has been shown experimentally in [ 31]i \\ndifferent domains. \\nSoftmax Softmax is often used as the last activation function of a neural \\nnetwork. In practice, it normalizes the output of a network to a \\nprobability distribution over the predicted output classes. Softmax is defined as: \\nSoftmaxdx iTh= ex \\ni \\nC \\nj ex \\nj :d9Th \\nThe softmax function takes as input a vector x of C real num-\\nbers and normalizes it into a probability distribution consisting of \\nC probabilities proportional to the exponentials of the input num-\\nbers. However, a limitation of softmax is that it assumes that every \\ninput x belongs to at least one of the C classes (which is not the case \\nin practice, i.e., the network could be applied to an input that does \\nnot belong to any of the classes). \\n2.3.3 Loss Functions Besides the activation functions, the loss function (which defines \\nthe cost function) is one of the main elements of neural networks. It is the function that represents the error for a given prediction. To \\nthat purpose, for a given training sample, it compares the prediction \\nf(x\\n(i) ;W) to the ground truth y(i) (here we denote for simplicity as \\nW all the parameters of the network, combining all the W1 , ..., WK \\nin the multilayer perceptron shown above). The loss is denoted as l(y, f(x;W)). The average loss across the n training samples is called \\nthe cost function and is defined as: \\nJdWTh= 1 \\nn n \\ni =1 l ydiTh,fdxdiTh;WTh , d10Th \\nwhere {( x(i) , y(i) )}i=1..n composes the training set. The aim of the \\ntraining will be to find the parameters W such that J(W) is mini-mized. Note that, in deep learning, one often calls the cost function \\nthe loss function, although, strictly speaking, the loss is for a given \\nsample, and the cost is averaged across samples. Besides, the objec-\\ntive function is the overall function to minimize, including the cost \\nand possible regularization terms. However, in the remainder of \\nthis chapter, in accordance with common usage in deep learning, \\nwe will sometimes use the term loss function instead of cost \\nfunction.',\n",
              "  '4.5 Classical\\nConvolutional Neural\\nNetwork Architecturesfrom all parts of it should have a receptive field close to the size of \\nthe input. The receptive field can be influenced by all the different \\nconvolution parameters and down-/upsampling operations \\ndescribed in the previous section. A comprehensive presentation \\nof mathematical derivations for calculating receptive fields for \\nCNNs is given in [ 52]. Deep Learning: Basics and CNN 107\\nIn the last decades, a variety of convolutional neural network archi-\\ntectures have been proposed. In this chapter, we cover only a few \\nclassical architectures for classification and regression. Note that \\nclassification and regression can usually be performed with the same architecture, just changing the loss function (e.g., cross-\\nentropy for classification, mean squared error for regression). \\nArchitectures for other tasks can be found in other chapters. \\nA Basic CNN Architecture Let us start with the most simple \\nCNN, which is actually very close to the original one proposed by \\nLeCun et al. [ 53], sometimes called \"LeNet.\" Such architecture is \\ntypically composed of two parts: the first one is based on convolu-tion operations and learns the features for the image and the second \\npart flattens the features and inputs them to a set of fully connected layers (in other words, a multilayer perceptron) for performing the \\nclassification/regression (see illustration in Fig. \\n18). Note that, of \\ncourse, the whole network is trained end to end: the two parts are \\nnot trained independently. In the first part, one combines a series of \\nblocks composed of a convolution operation (possibly strided \\nand/or dilated), a non-linear activation function (for instance, a \\nReLU), and a pooling operation. It is often a good idea to include a \\ndrawing of the different layers of the chosen architecture.\\nInput image Convolution \\n+ \\nNon-linearity Pooling Convolution \\n+ \\nNon-linearity Pooling \\nFeature learning Fully \\nconnected Flatten \\nClassification \\nFig. 18 A basic CNN architecture. Classically, it is composed of two main parts. The first one, using \\nconvolution operations, performs feature learning. The features are then flattened and fed into a set of fully \\nconnected layers (i.e., a multilayer perceptron), which performs the classification or the regression task',\n",
              "  \"Deep Learning: Basics and CNN 113\\n(2011) Theano: deep learning on GPUs with \\nPython. In: NIPS 2011, Big learning work-\\nshop, Granada, Spain, vol 3. Citeseer, pp 1-48 \\n13. Jia Y, Shelhamer E, Donahue J, Karayev S, \\nLong J, Girshick R, Guadarrama S, Darrell T \\n(2014) Caffe: convolutional architecture for fast feature embedding. In: Proceedings of the \\n22nd ACM international conference on Multi-\\nmedia, pp 675-678 \\n14. Abadi M, Agarwal A, Barham P, Brevdo E, \\nChen Z, Citro C, Corrado GS, Davis A, \\nDean J, Devin M et al (2016) TensorFlow: large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:160304467 \\n15. Chollet F et al (2015) Keras. https:/ /github. \\ncom/fchollet/keras \\n16. Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z, Gimelshein N, Antiga L et al (2019) PyTorch: \\nan imperative style, high-performance deep \\nlearning library. In: Advances in neural infor-\\nmation processing systems, vol 32 \\n17. Hebb DO (1949) The organization of behav-\\nior: a psychological theory. Wiley, New York \\n18. Cybenko G (1989) Approximations by super-positions of a sigmoidal function. Math Con-\\ntrol Signals Syst 2:183-192 \\n19. Hornik K, Stinchcombe M, White H (1989) \\nMultilayer feedforward networks are universal approximators. Neural Netw 2(5):359-366 \\n20. Mhaskar HN (1996) Neural networks for opti-\\nmal approximation of smooth and analytic \\nfunctions. Neural Comput 8(1):164-177 \\n21. Pinkus A (1999) Approximation theory of the \\nMLP model in neural networks. Acta Numer 8: \\n143-195 \\n22. Poggio T, Mhaskar H, Rosasco L, Miranda B, \\nLiao Q (2017) Why and when can deep-but \\nnot shallow-networks avoid the curse of \\ndimensionality: a review. Int J Autom Comput 14(5):503-519 \\n23. Rolnick D, Tegmark M (2017) The power of \\ndeeper networks for expressing natural func-tions. arXiv preprint arXiv:170505502 \\n24. Goodfellow I, Bengio Y, Courville A (2016) \\nDeep learning. MIT Press, Cambridge, MA \\n25. Cover TM (1965) Geometrical and statistical properties of systems of linear inequalities with \\napplications in pattern recognition. IEEE Trans Electron Comput 3:326-334 \\n26. Glorot X, Bordes A, Bengio Y (2011) Deep \\nsparse rectifier neural networks. In: Proceed-\\nings of the fourteenth international conference \\non artificial intelligence and statistics, JMLR \\nworkshop and conference proceedings, pp \\n315-323 27. Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet classification with deep convolu-tional neural networks. In: Advances in neural \\ninformation processing systems, vol 25 \\n28. Hein M, Andriushchenko M, Bitterwolf J (2019) Why ReLU networks yield high-confidence predictions far away from the train-\\ning data and how to mitigate the problem. In: \\nProceedings of the IEEE/CVF conference on \\ncomputer vision and pattern recognition, pp 41-50 \\n29. Maas AL, Hannun AY, Ng AY et al (2013) \\nRectifier nonlinearities improve neural network acoustic models. In: Proc. ICML, Atlanta, Georgia, vol 30. p 3 \\n30. He K, Zhang X, Ren S, Sun J (2015) Delving \\ndeep into rectifiers: surpassing human-level \\nperformance on ImageNet classification. In: \\nProceedings of the IEEE international confer-\\nence on computer vision, pp 1026-1034 \\n31. Ramachandran P, Zoph B, Le QV (2017) \\nSearching for activation functions. arXiv pre-\\nprint arXiv:171005941 \\n32. Dauphin YN, Pascanu R, Gulcehre C, Cho K, \\nGanguli S, Bengio Y (2014) Identifying and attacking the saddle point problem in high-\\ndimensional non-convex optimization. In: Advances in neural information processing sys-\\ntems, vol 27 \\n33. Bottou L (2010) Large-scale machine learning \\nwith stochastic gradient descent. In: Proceed-\\nings of COMPSTAT'2010. Springer, Berlin, \\npp 177-186 \\n34. Allen-Zhu Z, Li Y, Song Z (2019) A conver-\\ngence theory for deep learning via over-parameterization. In: International conference \\non machine learning, PMLR, pp 242-252 \\n35. Baydin AG, Pearlmutter BA, Radul AA, Siskind \\nJM (2018) Automatic differentiation in machine learning: a survey. J Mach Learn Res 18:1-43 \\n36. Prechelt L (1998) Early stopping-but when? In: \\nNeural networks: tricks of the trade. Springer, \\nBerlin, pp 55-69 \\n37. Reed R, MarksII RJ (1999) Neural smithing: \\nsupervised learning in feedforward artificial \\nneural networks. MIT Press, Cambridge, MA \\n38. Glorot X, Bengio Y (2010) Understanding the \\ndifficulty of training deep feedforward neural \\nnetworks. In: Proceedings of the thirteenth \\ninternational conference on artificial intelli-\\ngence and statistics, JMLR workshop and con-\\nference proceedings, pp 249-256 \\n39. Srivastava N, Hinton G, Krizhevsky A, \\nSutskever I, Salakhutdinov R (2014) Dropout: a simple way to prevent neural networks from\",\n",
              "  \"106 Maria Vakalopoulou et al.\\nFig. 17 Stride operation, here with a stride of 2 \\nTransposed Convolution In certain circumstances, one needs \\nnot only to downsample the spatial dimensions of the input but \\nalso, usually at a later stage of the network, apply an upsample \\noperation. The most emblematic case is the task of image segmen-\\ntation (see Chap. 13), in which a pixel-level classification is \\nexpected, and therefore, the output of the neural network should \\nhave the same size as the input. In such cases, several upsampling \\noperations are typically applied. The upsampling can be achieved by \\na transposed convolution operation that will eventually increase the \\nsize of the output. In details, the transposed convolution is per-formed by dilating the input instead of the kernel before applying a \\nconvolution operation. In this way, an input of size 5x5 will reach a size of 10x10 after being dilated with d=2. With proper padding and using a kernel of size 3x3, the output will eventually double \\nin size. \\n4.4 Receptive Field \\nCalculation In the context of deep neural networks and specifically CNNs, the \\nterm receptive field is used to define the proportion of the input \\nthat produces a specific feature. For example, a CNN that takes an \\nimage as input and applies only a single convolution operation with \\na kernel size of 3x3 would have a receptive field of 3x3. This \\nmeans that for each pixel of the first feature map, a 3x3 region of \\nthe input would be considered. Now, if another layer were to be \\nadded, with again 3x3 size, then the receptive field of the new \\nfeature map with respect to the CNN's input would be 5x5. In \\nother words, the proportion of the input that is used to calculate \\neach element of the feature map of the second convolution layer \\nincreases. \\nCalculating the receptive field at different parts of a CNN is \\ncrucial when trying to understand the inner workings of a specific architecture. For instance, a CNN that is designed to take as an \\ninput an image of size 256x256 and that requires information\",\n",
              "  'Additional\\n Architectural\\n Features of RNN\\nWe can use \\n additional\\n short\\n -\\ncut\\nconnections between \\n inputs\\n and \\n outputs\\n :\\n',\n",
              "  'Additional Architectural Features of RNN\\nWe can use higher\\n -\\norder states and connections between them,\\ne.g. the 2nd order states:\\n',\n",
              "  'Cell State of LSTMs\\nThe key to LSTM is the \\n ce\\nl\\nl state represented by the horizontal line \\nrunning through the top of the diagram. It is a kind of \\n conveyor belt\\n .\\nIt runs straight down the entire chain, with only some minor\\nlinear interactions.\\nThe LSTM has the ability to \\n remove\\n or \\nadd information \\nto the cell state\\n , carefully regulated by structures called \\n gates\\n .\\n',\n",
              "  'New Egg GPUs\\nyeah, I will add async endpoints, I swear (and I did)\\n1. Description\\nThis project is about scraping GPU from new egg, adding data to database, do operations on the data using api, running scraper with it as well and in the end deisplaying the data in dash web app.\\n\\n[![video](https://img.youtube.com/vi/Cw-WgAwtPD4/0.jpg)](https://www.youtube.com/watch?v=Cw-WgAwtPD4)\\n\\n2. Requirements\\n\\n  Python 3.9\\n  Packages from requirements.txt file\\n\\n\\n3. Components\\n\\n  Scraper\\n  Database\\n  Api\\n  Web app\\n\\n\\n\\n3.1 Scraper\\nThis is headless scraper made completely using requests that are sent asynchronously and using proxy in ip:port:user:pass format. Requests are made asynchronously to make whole process faster and proxy is used, so it won\\'t get blocked (wow). We focus on GPUs only that\\'s why it looks for a products in category id = 100007709, for example: https://www.newegg.com/p/pl?SrchInDesc=radeon+rx+580&N=100007709&PageSize=60.\\n\\n\\nBoth products details and reviews are being scraped, results are saved in 2 different json files (but in the same folder) in the sink folder. Folder with data is called data_{execution_id}_{intDateyyyymmdd}_{HHMMSSfff}, json files within data folder have the same time id and execution id but you replace data with Products/Reviews.\\n\\nParameters:\\n  phrase : int - phrase you look for, if there are is no such thing, scraper will save empty files (maybe not the best, but who cares)\\n  max_pages : int, default: 0 - when set to 0, it will take all pages\\n  execution_id : str, default: \"0\" - when using api it\\'s generated automatically, when using scraper you can provide whatever int you want\\n  save : bool, default: True - saving to sink folder. BTW in api you don\\'t have a choice XD it\\'s always true cuz I forgot and didn\\'t care lmao\\n\\n\\n\\n3.2 Database\\nAs database I used sqlalchemy(sqlite), it contains the following tables: Products, Reviews, SpeschulUsers. See database/models.py\\n\\n  Products - stores products, it has relationship with Reviews table. This table contains historical data of products, Archive column is really important here - when record has Archived set to false, then this is the most recent records, if not it\\'s historical record.\\n  Products - stores reviews, it has relationship with Products table. ProductId is a foreign key.\\n  SpeschulUsers - stores users with special roles, for now it has only Admin user with admin role (admin is the only role), becuase most of endpoints in API are protected by password, so you use this user to authenticate. \\n\\n\\nDb services - database/db_services.py\\nTo simplify things, I\\'ve created function to perform different operations on database (and sink folder), like delete, insert, update, select. These operations (except of insert) have 2 functions each - one uses ORM and the other gives user more freedom when it comes to setting WHERE clause and SELECT, COUNTs, SUMs GROUP BY and other stuff\\n\\n![Screenshot_3](https://github.com/Koks-creator/NewEggGPUs/assets/73878161/079d515b-e3a7-4aa9-8159-79d604d6122c)\\n\\n![Screenshot_4](https://github.com/Koks-creator/NewEggGPUs/assets/73878161/d749e0eb-361d-404c-8690-0acc465b2a12)\\n\\nWhen it comes to insert, I wanted it to be inserted only from scraped data (so no manual insert unless you do it directly on database by creating your own query). Data is added using add_data_from_sink. Products are being checked for duplicates to avoid duplicated record and checks if some column is different, if in db we have product with id \\'MYID\\' and product with same id has been scraped but price has changed then old record is being archived (Archived set to true) and new active (Archived set to false) record is being created. Reviews are being checked only for duplicates.\\n\\n\\nDatabase is being created in setup.py file and it also includes creating admin user\\n\\n3.3 API\\nIt\\'s used for db operations, running scraper, managing sink folder. It works on port 8000. I\\'m too lazy to describe all of these endpoints and schemas, just see /docs endpoint and read goddam code.\\n\\n![Screenshot_5](https://github.com/Koks-creator/NewEggGPUs/assets/73878161/4f5a8524-9892-4ab4-8af3-e9813214a98f)\\n\\n![Screenshot_6](https://github.com/Koks-creator/NewEggGPUs/assets/73878161/b67a85a1-7470-469a-af09-8bc5d1d3e236)\\n\\n3.4 Web app\\nWeb app made in dash (superapp/gpu_app.py), you\\'ve probably saw how it works and looks like cuz there is video at the top of this readme. It\\'s used for visualizing data, that\\'s it. Oh, and it works on port 8050 (I guess it\\'s a default dash port).\\n\\n\\n',\n",
              "  'Introduction to NLP and Words Representation\\n\\nNatural Language Processing (NLP) \\n includes various tasks of language analysis, understanding, \\ntranslation, generation, classification and clustering, so we need to operate on words.\\n\\nWe\\nusually\\n use\\nany\\n kind\\n of\\na\\nword dictionary (a vocabulary of the processed language) and \\neach word from the given vocabulary can be represented as \\n a one\\n -\\nhot vector\\n , which is the \\nvector consisting of zeros except to the single position representing a given word equal to 1:\\n\\nThe\\n one\\n-\\nhot\\nvectors are often used\\n to\\nrepresent a sequence of words on the\\n inputs\\n of\\nthe recurrent neural networks as well as other types of neural networks.\\nNo Dictionary\\n1 a\\n...\\n982 and\\n...\\n132847 word\\n...\\n171476 zyzzyvaand\\n0\\n...\\n1\\n...\\n0\\n...\\n0word\\n0\\n...\\n0\\n...\\n1\\n...\\n0a\\n1\\n...\\n0\\n...\\n0\\n...\\n0zyzzyva\\n0\\n...\\n0\\n...\\n0\\n...\\n1',\n",
              "  'perceptron is calculated. If the prediction is correct, no further \\naction is needed, and the next data point is processed. If the prediction is wrong, the weights are updated with the \\nfollowing rule: the weights are increased in case the prediction is \\nsmaller than the ground-truth label y\\n(i) and decreased if the predic-\\ntion is higher than the ground-truth label. This process is repeated \\nuntil no further errors are made for the data points. A pseudocode \\nof the training or con vergence a lgorithm is presented in \\nAlgorithm 1 (note that in this version, it is assumed that the data \\nis linearly separable). 80 Maria Vakalopoulou et al.\\nx1 \\nx2 \\nxp 1\\n/g54 ^y \\nwp w2 w1 b \\nFig. 1 A simple perceptron model. The input elements are described as neurons \\nand combined for the final prediction ^y. The final prediction is composed of a \\nweighted sum and an activation function \\nAlgorithm 1 Train perceptron \\nprocedure Train({(x(i),y(i))}) \\nInitialization: initialize randomly the weights w and bias b \\nwhile i {1,...,n},f(x(i);w,b)= y(i) do \\nPick i randomly \\nerror = y(i)- f(x(i);w,b) \\nif error=0 then \\nw - w + error* x(i) \\nb b + error \\nOriginally, the perceptron has been proposed for binary classi-\\nfication tasks. However, this algorithm can be generalized for the case of multiclass classification, f\\nc(x;w, b), where c{1, ..., C} are \\nthe different classes. This can be easily achieved by adding more neurons to the output layer of the perceptron. That way, the number of output neurons would be the same as the number of \\npossible outputs we need to predict for the specific problem. Then, the final decision can be made by choosing the maximum of the \\ndifferent output neurons f \\nn = max f cdx;w,bTh. \\ncf1,...,CgFinally, in the following, we will integrate the bias b in the \\nweights w (and thus add 1 as the first element of the input vector x=[1, x\\n1, ..., x p] ). The model can then be rewritten as f(x;w) such \\nthat fd:;wTh : xpth1 -fth1,-1g.',\n",
              "  \"HAL Id: hal-03957224\\nhttps://hal.science/hal-03957224v2\\nSubmitted on 3 Oct 2023\\nHAL is a multi-disciplinary open access\\narchive for the deposit and dissemination of sci-\\nentific research documents, whether they are pub-\\nlished or not. The documents may come from\\nteaching and research institutions in F rance or\\nabroad, or from public or private research centers.L'archive ouverte pluridisciplinaire HAL , est\\ndestinee au depot et a la diffusion de documents\\nscientifiques de niveau recherche, publies ou non,\\nemanant des etablissements d'enseignement et de\\nrecherche francais ou etrangers, des laboratoires\\npublics ou prives.\\nDistributed under a Creative Commons Attribution 4.0 International License\\nDeep learning: basics and convolutional neural networks\\n(CNN)\\nMaria V akalopoulou, Stergios Christodoulidis, Ninon Burgos, Olivier Colliot,\\nVincent Lepetit\\nT o cite this version:\\nMaria V akalopoulou, Stergios Christodoulidis, Ninon Burgos, Olivier Colliot, Vincent Lepetit. Deep\\nlearning: basics and convolutional neural networks (CNN). Olivier Colliot. Machine Learning for\\nBrain Disorders, Springer, 2023, 10.1007/978-1-0716-3195-9_3. hal-03957224v2\",\n",
              "  '86 Maria Vakalopoulou et al.\\nNote that this is in fact the logistic function, which is a special \\ncase of the more general class of sigmoid function. As it is indicated \\nin Fig. 5b, the sigmoid gradient vanishes for large or small inputs \\nmaking the training process difficult. However, in case it is used for \\nthe output units which are not latent variables and on which we have access to the ground-truth labels, sigmoid may be a good \\noption. \\nRectified Linear Unit (ReLU) ReLU is considered among the default choice of non-linearity. \\nSome of the main advantages of ReLU include its efficient calcula-\\ntion and better gradient propagation with fewer vanishing gradient \\nproblems compared to the previous two activation functions \\n[26]. Formally, the ReLU function, together with its gradient, is \\ndefined as: \\ng = maxd0,xTh \\ng \\nx = 0, if x <=0 \\n1,if x >0 :d7Th \\nAs it is indicated in Fig. 5c, ReLU is differentiable anywhere \\nelse than zero. However, this is not a very important problem as the \\nvalue of the derivative at zero can be arbitrarily chosen to be 0 or \\n1. In [ 27], the authors empirically demonstrated that the number \\nof iterations required to reach 25% training error on the CIFAR-10 \\ndataset for a four-layer convolutional network was six times faster \\nwith ReLU than with tanh neurons. On the other hand, and as \\ndiscussed in [ 28], ReLU-type neural networks which yield a piece-\\nwise linear classifier function produce almost always high confi-\\ndence predictions far away from the training data. However, due \\nto its efficiency and popularity, many variations of ReLU have been \\nproposed in the literature, such as the leaky ReLU [ 29] or the \\nparametric ReLU [ 30]. These two variations both address the \\nproblem of dying neurons, where some ReLU neurons die for all \\ninputs and remain inactive no matter what input is supplied. In such \\na case, no gradient flows from these neurons, and the training of the neural network architecture is affected. Leaky ReLU and parametric \\nReLU change the g(x)=0 part, by adding a slope and extending the range of ReLU. \\nSwish The choice of the activation function in neural networks is not \\nalways easy and can greatly affect performance. In [ 31], the authors \\nperformed a combination of exhaustive and reinforcement \\nlearning-based searches to discover novel activation functions. \\nTheir experiments discovered a new activation function that is \\ncalled Swish and is defined as:',\n",
              "  'Long Short\\n -\\nTerm Memory (Vanilla LSTM)\\nPeepholes connections of \\n Vanilla LSTM allow directly controlling all gates to easier \\nlearn precise timings, supporting full backpropagation through time training:\\n',\n",
              "  'only capable of learning linearly separable patterns. It was often \\nincorrectly believed that they also presumed this is the case for \\nmultilayer perceptron networks. It took more than 10 years for \\nresearch on neural networks to recover, and in [ 3], some of these \\nissues were clarified and further discussed. Even if during this \\nperiod there was not a lot of research interest for perceptrons, \\nvery important algorithms such as the backpropagation algorithm \\n[4-7] and recurrent neural networks [ 8] were introduced.78 Maria Vakalopoulou et al.\\nAfter this period, and in the early 2000s, publications by Hin-\\nton, Osindero, and Teh [ 9] indicated efficient ways to train multi-\\nlayer perceptrons layer by layer, treating each layer as an \\nunsupervised restricted Boltzmann machine and then using super-\\nvised backpropagation for the fine-tuning [ 10]. Such advances in \\nthe optimization algorithms and in hardware, in particular graphics \\nprocessing units (GPUs), increased the computational speed of \\ndeep learning systems and made their training easier and faster. Moreover, around 2010, the first large-scale datasets, with Ima-geNet [\\n11] being one of the most popular, were made available, \\ncontributing to the success of deep learning algorithms, allowing \\nthe experimental demonstration of their superior performance on several tasks in comparison with other commonly used machine \\nlearning algorithms. Finally, another very important factor that contributed to the current popularity of deep learning techniques \\nis their support by publicly available and easy-to-use libraries such \\nas Theano [\\n12], Caffe [ 13], TensorFlow [ 14], Keras [ 15], and \\nPyTorch [ 16]. Indeed, currently, due to all these publicly available \\nlibraries that facilitate collaborative and reproducible research and \\naccess to resources from large corporations such as Kaggle, Google \\nColab, and Amazon Web Services, teaching and research about \\nthese algorithms have become much easier. \\nThis chapter will focus on the presentation and discussion of \\nthe main components of deep learning algorithms, giving the \\nreader a better understanding of these powerful models. The chap-\\nter is meant to be readable by someone with no background in deep \\nlearning. The basic notions of machine learning will not be \\nincluded here; however, the reader should refer to Chap. 2 (reader \\nwithout a background in engineering or computer science can also \\nrefer to Chap. 1 for a lay audience-oriented presentation of these \\nconcepts). The rest of this chapter is organized as follows. We will \\nfirst present the deep feedforward networks focusing on percep-\\ntrons, multilayer perceptrons, and the main functions that they are \\ncomposed of (Subheading 2). Then, we will focus on the optimiza-\\ntion of deep neural networks, and in particular, we will formally \\npresent the topics of gradient descent, backpropagation, as well as \\nthe notions of generalization and overfitting (Subheading 3). Sub-\\nheading 4 will focus on convolutional neural networks discussing in \\ndetail the basic convolution operations, while Subheading 5 will \\ngive an overview of the autoencoder architectures.',\n",
              "  'Back\\n-\\nPropagation \\n Through Time (BPTT)\\nThe \\n backpropagation algorithm\\n can be \\n adapted\\n to sequential patterns:\\n',\n",
              "  'empirical distribution of the training set and d is the l 2 norm, \\nEq. 30 is equivalent to Eq. 29.Deep Learning: Basics and CNN 111\\nMany variations of autoencoders exist, to prevent autoencoders \\nfrom learning the identity function and to improve their ability to \\ncapture important information and learn richer representations. \\nAmong them, sparse autoencoders offer an alternative method for \\nintroducing an information bottleneck without requiring a reduc-tion in the number of nodes at the hidden features. This is done by \\nconstructing the loss function such that it penalizes activations \\nwithin a layer. This is achieved by enforcing sparsity in the network \\nand encouraging it to learn an encoding and decoding which relies \\nonly on activating a small number of neurons. This sparsity is \\nenforced in two main ways, an l\\n1 regularization on the parameters \\nof the network and a Kullback-Leibler divergence, which is a mea-\\nsure of the difference between two probability distributions. More \\ninformation about sparse autoencoders could be found in \\n[70]. Moreover, a quite common type of autoencoders is the \\ndenoising autoencoders [ 71], on which the model is tasked with \\nreproducing the input as closely as possible while passing through \\nsome sort of information bottleneck (Fig. 20). This way, the model \\nis not able to simply develop a mapping that memorizes the training \\ndata but rather learns a vector field for mapping the input data \\ntoward a lower-dimensional manifold. One should note here that the vector field is typically well-behaved in the regions where the \\nmodel has observed data during training. In out-of-distribution \\ndata, the reconstruction error is both large and does not always point in the direction of the true distribution. This observation \\nmakes these networks quite popular for anomaly detection in med-ical data [\\n72]. Additionally, contractive autoencoders [ 73] are other \\nvariants of this type of models, adding the contractive regulariza-\\ntion loss to the standard autoencoder loss. Intuitively, it forces very \\nsimilar inputs to have a similar encoding, and in particular, it \\nrequires the derivative of the hidden layer activations to be small \\nwith respect to small changes in the input. The denoising autoen-\\ncoders can be understood as a variation of the contractive autoen-\\ncoder. In the limit of small Gaussian noise, the denoising \\nautoencoders make the reconstruction error resistant to finite-\\nsized input perturbations, while the contractive autoencoders \\nmake the extracted features resistant to small input perturbations. \\nDepending on the input type, different autoencoder architec-\\ntures could be designed. In particular, when the inputs are images, the encoder and the decoder are classically composed of convolu-\\ntional blocks. The decoder uses, for instance, transposed convolu-tions to perform the expansion. Finally, the addition of skip \\nconnections has led to the U-Net [\\n74] architectures that are com-\\nmonly used for segmentation purposes. Segmentation architectures will be more extensively described in Chap. \\n13. Finally, variational \\nautoencoders, which rely on a different mathematical formulation,',\n",
              "  'gradient descent will be OdKTh and for gradient descent OdNTh. The \\nideal choice for the batch size is a debated question. First, an upper \\nlimit for the batch size is often simply given the available GPU \\nmemory, in particular when the size of the input data is large (e.g., \\n3D medical images). Besides, choosing K as a power of 2 often \\nleads to more efficient computations. Finally, small batch sizes tend \\nto have a regularizing effect which can be beneficial [ 24]. In any \\ncase, the ideal batch size usually depends on the application, and it \\nis not uncommon to try different batch sizes. Finally, one calls an \\nepoch a complete pass over the whole training set (meaning that each training sample has been used once). The number of epochs is \\nthe number of full passes over the whole training set. It should not \\nbe confused with the number of iterations which is the number of mini-batches that have been processed.Deep Learning: Basics and CNN 91\\nNote that various improvements over traditional SGD have \\nbeen introduced, leading to more efficient optimization methods. These state-of-the-art optimization methods are presented in \\nSubheading \\n3.4. \\nBox 2: Convergence of SGD Theorem \\nIn [ 34], the authors prove that stochastic gradient \\ndescent converges if the network is sufficiently overpara-\\nmetrized. Let (x(i) , y(i) )1<=i<=n be a training set satisfying \\nmin i,j:i  j kx(i)-x( j)k2>d> 0. Consider fitting the data \\nusing a feedforward neural network with ReLU activa-\\ntions. Denote by D (resp. W) the depth (resp. width) of \\nthe network. Suppose that the neural network is suffi-\\nciently overparametrized, i.e.: \\nW polynomial n,D, 1 \\nd :d17Th \\nThen, with high probability, running SGD with some random \\ninitialization and properly chosen step sizes e t yields J(Wt ) \\n<E in t / log 1 \\ne. \\n3.2 Backpropagation The training of neural networks is performed with backpropaga-tion. Backpropagation computes the gradient of the loss function with respect to the parameters of the network in an efficient and \\nlocal way. This algorithm was originally introduced in 1970. How-\\never, it started becoming very popular after the publication of [\\n6], \\nwhich indicated that backpropagation works faster than other \\nmethods that had been proposed back then for the training of \\nneural networks.',\n",
              "  \"the model. Drop-out is an ensemble method that does not need to \\nbuild the models explicitly. In practice, at each optimization itera-\\ntion, random binary masks on the units are considered. The proba-\\nbility of removing a unit (p) is defined as a hyperparameter during \\nthe training of the network. During inference, all the units are \\nactivated; however, the obtained parameters W are multiplied with this probability p. Drop-out is quite efficient and commonly \\nused in a variety of neural network architectures. Deep Learning: Basics and CNN 95\\nFig. 9 Examples of data transformations applied in the MNIST dataset. Each of these generated samples is \\nconsidered additional training data \\nData Augmentation Since neural networks are data-driven meth-\\nods, their performance depends on the training data. To increase \\nthe amount of data during the training, data augmentation can be performed. It generates slightly modified copies of the existing \\ntraining data to enrich the training samples. This technique acts as a regularizer and helps reduce overfitting. Some of the most com-monly used transformations applied during data augmentation \\ninclude random rotations, translations, cropping, color jittering, resizing, Gaussian blurring, and many more. In Fig. \\n9, examples \\nof different transformations on different digits (first column) of the \\nMNIST dataset [ 40] are presented. For medical images, the \\nTorchIO library allows to easily perform data augmentation [ 41]. \\nBatch N orma lization To ensure that the training of the networks \\nwill be more stable and faster, batch normalization has been pro-\\nposed [ 42]. In practice, batch normalization re-centers and \\nre-scales the layer's input, mitigating the problem of internal\",\n",
              "  'Additional Architectural Features of RNN\\nWe can also force the target signal (presented by a teacher):\\n',\n",
              "  'However, all these minima are often almost equivalent to each \\nother in cost function value. In that case, these local minima are \\nnot a problematic form of non-convexity. It remains an open ques-\\ntion whether there exist many local minima with a high cost that \\nprevent adequate training of neural networks. However, it is cur-\\nrently believed that most local minima, at least as found by modern \\noptimization procedures, will correspond to a low cost (even though not to identical c osts) [\\n24].90 Maria Vakalopoulou et al.\\nFor W/C3to be a local minimum, we need mainly two conditions \\nto be fulfilled:\\nJ \\nW dW/C3Th =0.\\nAll the eigenvalues of 2 J \\nW2dW/C3Th to be positive. \\nFor random functions in n dimensions, the probability for the \\neigenvalues to be all positive is 1 \\nn. On the other hand, the ratio of the \\nnumber of saddle points to local minima increases exponentially with \\nn [32]. A saddle point, or critical point, is a point where the deriva-\\ntives are zero without being a minimum of the function. Such points \\ncould result in a high error making the optimization with gradient \\ndescent challenging. In [ 32], this issue is discussed, and an optimi-\\nzation algorithm that leverages second-order curvature information \\nis proposed to deal with this issue for deep and recurrent networks. \\n3.1.1 Stochastic Gradient \\nDescent Gradient descent efficiency is not enough when it comes to \\nmachine learning problems with large numbers of training samples. Indeed, this is the case for neural networks and deep learning which \\noften rely on hundreds or thousands of training samples. Updating the parameters W after calculating the gradient using all the \\ntraining samples would lead to a tremendous computational com-\\nplexity of the underlying optimization algorithm [\\n33]. To deal with \\nthis problem, the stochastic gradient descent (SGD) algorithm is a \\ndrastic simplification. Instead of computing the JdWTh \\nW exactly, each \\niteration estimates this gradient on the basis of a small set of randomly picked examples, as follows: \\nWtth1 -Wt-etGdWtTh, d15Th \\nwhere \\nGdWtTh= 1 \\nK K \\nk=1 JdikThWt \\nW , d16Th \\nwhere Jik is the loss function at training sample i k, \\nfdxdikTh, ydikThThgk=1...K is the small subset of K training samples \\n(K<<N). This subset of K samples is called a mini-batch or sometimes a batch.\\n2 In such a way, the iteration cost of stochastic\\n2 Note that, as often in deep learning, the terminology can be confusing. In isolation, the term batch is usually a \\nsynonym of mini-batch. On the contrary, batch gradient descent means computing the gradient using all training \\nsamples and not only a mini-batch [ 24].',\n",
              "  'which the input neurons are fed into a hidden layer whose neurons \\nare combined for the final prediction.82 Maria Vakalopoulou et al.\\nThere were a lot of research works indicating the capacity of \\nfeedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In the late 1980s, the first \\nproof was published [\\n18] for sigmoid activation functions (see \\nSubheading 2.3 for the definition) and was generalized to other \\nfunctions for feedforward multilayer architectures [ 19-21]. In par-\\nticular, these works prove that any continuous function can be approximated under mild conditions as closely as wanted by a \\nthree-layer network. As N -1, any continuous function f can \\nbe approximated by some neural network ^f , because each compo-\\nnent gdW\\nT \\ndjThxTh behaves like a basis function and functions in a \\nsuitable space admit a basis expansion. However, since N may \\nneed to be very large, introducing some limitations for these \\ntypes of networks, deeper networks, with more than one hidden \\nlayer, can provide good alternatives. \\n2.2.2 Deep Neural \\nNetwork The simple MLP networks can be generalized to deeper networks \\nwith more than one hidden layer that progressively generate \\nhigher-level features from the raw input. Such networks can be \\nwritten as: \\nz1dxTh=gdW1 xTh \\n... \\nzkdxTh=gdWk zk-1dxThTh \\n... \\n^y =fdx;W1 ,...,WKTh=z KdzK-1d...dz 1dxThThThTh , d4Th \\nwhere K denotes the number of layers for the neural network, \\nwhich defines the depth of the network. In Fig. 3, a graphical \\nrepresentation of the deep multilayer perceptron is presented. \\nOnce again, the input layer is fed into the different hidden layers \\nof the network in a hierarchical way such that the output of one \\nlayer is the input of the next one. The last layer of the network \\ncorresponds to the output layer, which makes the final prediction of \\nthe model. \\nAs for networks with one hidden layer, they are also universal \\napproximators. However, the approximation theory for deep net-\\nworks is less understood compared with neural networks with one \\nhidden layer. Overall, deep neural networks excel at representing \\nthe composition of functions. \\nSo far , we have described neural networks as simple chains of \\nlayers, applied in a hierarchical way, with the main considerations being the depth of the network (the number of layers K) and the',\n",
              "  'State Transition Function\\nThe state transition function defining a single time step can be defined by \\nthe shift operator q\\n-\\n1\\n:\\n*\\nh\\n0 \\n-\\nan initial step (at t=0) associated with the external vertex (\\n frontier\\n )\\n*\\nh\\nt\\n= f(h\\nt\\n-\\n1\\n, \\nx\\nt\\n) \\n-\\nt\\n-\\nstep\\n*\\nq\\n-\\n1\\nh\\nt\\n= h\\nt\\n-\\n1 \\n-\\nunitary time delay\\n*\\no\\nt\\n-\\noutput (predicted value)\\n',\n",
              "  'Comparison of BPTT and RTRL\\nBoth BPTT and RTRL compute the same gradients but in different ways.\\nThey differ in computational complexity:\\n',\n",
              "  'Long Short\\n -\\nTerm Memory (LSTM)\\nIn the first step, the LSTM decides by a sigmoid layer called the \"forget gate \\nlayer\" what information is let to go throw away from the cell state. \\nThe \\n forget gate (o) \\n of a simple LSTM cell takes \\n the decision \\n about what must be \\nremoved from the h\\nt\\n-\\n1\\nstate after getting the output of the previous state, \\nand it thus keeps only the relevant stuff. It is surrounded by a sigmoid function \\n \\nwhich crushes the input between [0, 1]. \\nWe multiply \\n the forget \\n gate with previous cell state to forget the unnecessary \\nstuff from the previous state which is not needed anymore.\\n',\n",
              "  'Back\\n-\\nPropagation \\n Through Time (BPTT)\\nThe \\n backpropagation algorithm\\n can be \\n adapted\\n to sequential patterns:\\n',\n",
              "  'Recurrent Neural Networks and Long Short\\n -\\nTerm Memory\\nfor \\nLearning \\n Sequences and Natural Language Processing\\nAGH \\n University\\n of\\nScience and Technology\\nKrakow, Poland\\nAdrian Horzyk\\nhorzyk@agh.edu.pl\\nCOMPUTATIONAL\\n INTELLIGENCE\\nDEEP LEARNING\\n',\n",
              "  \"Long Short\\n -\\nTerm Memory (LSTM)\\nIn the third step, the LSTM updates the \\n old cell state C\\nt-1\\ninto the new cell state C\\nt\\n. \\nThe previous steps already decided what to do, we just need to actually do it.\\nWe multiply the old state by f\\nt\\n, forgetting the things we decided to forget earlier. \\nThen we add \\n i\\nt\\n*\\nC\\nt\\n. This is the new candidate values, scaled by how much we \\ndecided to update each state value.\\nWe can \\n actually drop the information about the old subject's \\n attribute and \\n add \\nthe new information, as we decided in the previous steps.\\n\",\n",
              "  '2j=0jare homogeneously, linearly separable.Most of the time, the best architecture is defined empirically. In the \\nnext section, we will discuss the main functions used in neural \\nnetworks. 84 Maria Vakalopoulou et al.\\n2.3 Main Functions A neural network is a composition of different functions also called \\nmodules. Most of the times, these functions are applied in a sequen-tial way. However, in more complicated designs (e.g., deep residual \\nnetworks), different ways of combining them can be designed. In \\nthe following subsections, we will discuss the most commonly used \\nfunctions that are the backbones of most perceptrons and multi-\\nlayer perceptron architectures. One should note, however, that a \\nvariety of functions can be proposed and used for different deep \\nlearning architectures with the constraint to be differentiable - \\nalmost - everywhere. This is mainly due to the way that deep neural \\nnetworks are trained, and this will be discussed later in the chapter. \\n2.3.1 Linear Functions One of the most fundamental functions used in deep neural net-\\nworks is the simple linear function. Linear functions produce a linear combination of all the nodes of one layer of the network, \\nweighted with the parameters W. The output signal of the linear function is Wx, which is a polynomial of degree one. While it is easy \\nto solve linear equations, they have less power to learn complex \\nfunctional mappings from data. Moreover, when the number of \\nsamples is much larger than the dimension of the input space, the \\nprobability that the data is linearly separable comes close to zero \\n(Box \\n1). This is why they need to be combined with non-linear \\nfunctions, also called activation functions (the name activation has been initially inspired by biology as the neuron will be active or not \\ndepending on the output of the function). \\nBox 1: Function Counting Theorem \\nThe so-called Function Counting Theorem (Cover [ 25]) \\ncounts the number of linearly separable dichotomies of \\nn points in general position in p . The theorem shows that, \\nout of the total 2n dichotomies, only Cdn,pTh= \\np n-1 \\nWhen n>>p, the probability of a dichotomy to be line-\\narly separable converges to zero. This indicates the need for the integration of non-linear functions into our modeling and \\narchitecture design. Note that n>>p is a typical regime in \\nmachine learning and deep learning applications where the \\nnumber of samples is very large.',\n",
              "  'a98 Maria Vakalopoulou et al.\\ngt -GdWtTh \\nst -r1st-1 thd1-r1Thgt \\nrt -r2rt-1 thd1-r2Thgt gt \\n^st - st \\n1-dr1Tht \\n^rt - rt \\n1-dr2Tht \\nDWt --l \\ndth ^rtp ^st \\nWtth1 -Wt thDWt ,d24Th \\nwhere st is the gradient with momentum, rt accumulates the \\nsquared gradients with momentum as in RMSProp, and ^st and ^rt \\nare smaller than st and rt , respectively, but they converge toward \\nthem. Moreover, d is some small quantity that is used to avoid the \\ndivision by zero, while r 1 and r 2 are hyperparameters of the algo-\\nrithm. The parameters r 1 and r 2 control the decay rates of each \\nmoving average, respectively, and their value is close to 1. Empirical \\nresults demonstrate that Adam works well in practice and compares \\nfavorably to other stochastic optimization methods, making it the \\ngo-to optimizer for deep learning problems. \\n3.4.5 Other Optimizers The development of efficient (in terms of speed and stability) \\noptimizers is still an active research direction. RAdam [ 49]i s \\nvariant of Adam, introducing a term to rectify the variance of the \\nadaptive learning rate. In particular, RAdam leverages a dynamic \\nrectifier to adjust the adaptive momentum of Adam based on the variance and effectively provides an automated warm-up custom-\\ntailored to the current dataset to ensure a solid start to training. \\nMoreover, LookAhead [\\n50] was inspired by recent advances in the \\nunderstanding of loss surfaces of deep neural networks and pro-vides a breakthrough in robust and stable exploration during the \\nentirety of the training. Intuitively, the algorithm chooses a search \\ndirection by looking ahead at the sequence of fast weights gener-\\nated by another optimizer. These are only some of the optimizers \\nthat exist in the literature, and depending on the problem and the \\napplication, different optimizers could be selected and applied. \\n4 Convolutional Neural Networks \\nConvolutional neural networks (CNNs) are a specific category of \\ndeep neural networks that employ the convolution operation in \\norder to process the input data. Even though the main concept \\ndates back to the 1990s and is greatly inspired by neuroscience [ 51] \\n(in particular by the organization of the visual cortex), their wide-\\nspread use is due to a relatively recent success on the ImageNet \\nLarge Scale Visual Recognition Challenge of 2012 [ 27]. In contrast',\n",
              "  'LEARNING SEQUENCES\\nSequences\\n usually \\n model processes in time (actions, movements)\\n and are \\nsequentially processed in time to predict next data (conclusions, reactions).\\nSequences\\n can have \\n variable length \\n but typical machine learning models \\nuse a fixed number of inputs (fixed\\n -\\nsize window) as a prediction context:\\n',\n",
              "  \"Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International\\nLicense ( ), which permits use, sharing, adaptation, distribution\\nand reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the\\nsource, provide a link to the Creative Commons license and indicate if changes were made. The images or otherthird party material in this chapter are included in the chapter's Creative Commons license, unless indicated\\notherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and\\nyour intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtainpermission directly from the copyright holder.http:/ /creativecommons.org/licenses/by/4.0/Deep Learning: Basics and CNN 115\\n64. Wen J, Thibeau-Sutre E, Diaz-Melo M, Sam-\\nper\\n-Gonza 'lez J, Routier A, Bottani S, \\nDormont D, Durrleman S, Burgos N, Colliot \\nO (2020) Convolutional neural networks for \\nclassification of Alzheimer's disease: overview \\nand reproducible evaluation. Med Image Anal \\n63:101694 \\n65. Chen S, Ma K, Zheng Y (2019) Med3D: trans-\\nfer \\nlearning for 3D medical image analysis. \\narXiv preprint arXiv:190400625 \\n66. Tan M, Le Q (2019) EfficientNet: rethinking \\nmodel \\nscaling for convolutional neural \\nnetworks. In: International conference on \\nmachine learning, PMLR, pp 6105 -6114 \\n67. Wang J, Liu Q, Xie H, Yang Z, Zhou H (2021) \\nBoosted \\nEfficientNet: detection of lymph node \\nmetastases in breast cancer using convolutional neural networks. Cancers 13(4):661 \\n68. Oloko-Oba M, Viriri S (2021) Ensemble of \\nEfficientNets \\nfor the diagnosis of tuberculosis. \\nComput Intell Neurosci 2021:9790894 \\n69. Ali K, Shaikh ZA, Khan AA, Laghari AA (2021) \\nMulticlass \\nskin cancer classification using \\nEfficientNets --a first step towards preventing \\nskin cancer. Neurosci Inform 2(4):100034 70. Ng A et al (2011) Sparse autoencoder. CS294A Lecture \\nNotes 72(2011):1 -19 \\n71. Vincent P, Larochelle H, Bengio Y, Manzagol P\\nA (2008) Extracting and composing robust \\nfeatures with denoising autoencoders. In: Pro-\\nceedings of the 25th international conference \\non machine learning, pp 1096 -1103 \\n72. Baur C, Denner S, Wiestler B, Navab N, Albar-\\nqouni \\nS (2021) Autoencoders for unsupervised \\nanomaly segmentation in brain MR images: a \\ncomparative study. Med Image Anal 69: \\n101952 \\n73. Salah R, Vincent P, Muller X, et al (2011) \\nContractive \\nauto-encoders: explicit invariance \\nduring feature extraction. In: Proceedings of \\nthe 28th international conference on machine \\nlearning, pp 833 -840 \\n74. Ronneberger O, Fi scher P, Brox T (2015) \\nU-net: convolutional networks for biomedical \\nimage segmentation. In: International Confer-\\nence on Medical image computing and \\ncomputer-assisted intervention. Springer, Ber-lin, pp 234 -241\",\n",
              "  '3.3 Generalization\\nand Overfitting\\nTh92 Maria Vakalopoulou et al.\\nFig. 7 A multilayer perceptron with one hidden layer \\nThe backpropagation algorithm works by computing the gra-\\ndient of the loss function (J) with respect to each weight by the \\nchain rule, computing the gradient one layer at a time, and iterating \\nbackward from the last layer to avoid redundant calculations of \\nintermediate terms in the chain rule. In Fig. 7, an example of a \\nmultilayer perceptron with one hidden layer is presented. In such a \\nnetwork, the backpropagation is calculated as: \\nJdWTh \\nw2 = JdWTh \\n^y x ^y \\nw2 \\nJdWTh \\nw1 = JdWTh \\n^y x ^y \\nw1 = JdWTh \\n^y x ^y \\nz1 x z1 \\nw1 : d18Th \\nOverall, backpropagation is very simple and local. However, \\nthe reason why we can train a highly non-convex machine with many local minima, like neural networks, with a strong local \\nlearning algorithm is not really known even today. In practice, \\nbackpropagation can be computed in different ways, including \\nmanual calculation, numerical differentiation using finite difference \\napproximation, and symbolic differentiation. Nowadays, deep \\nlearning frameworks such as [\\n14, 16] use automatic differentiation \\n[35] for the application of backpropagation. \\nSimilar to all the machine learning algorithms (discussed in \\nChapter 2), neural networks can suffer from poor generaliza-\\ntion and overfitting. These problems are caused mainly by the \\noptimization of the parameters of the models performed in the {(x\\ni, yi)}i=1,.. .,n training set, while we need the model to per-\\nform well on other unseen data that are not available during the \\ntraining. More formally, in the case of cross-entropy, the loss that we would like to minimize is: \\nJdWTh=-log  dx,yThT T P y=yjx=x;Wd, d19Th \\nwhere T T is the set of any data, not available during training. In \\npractice, a small validation set T V is used to evaluate the loss on \\nunseen data. Of course, this validation set should be distinct from the training set. It is extremely important to keep in mind that the \\nperformance obtained on the validation set is generally biased \\nupward because the validation set was used to perform early stop-\\nping or to choose regularization parameters. Therefore, one should \\nhave an independent test set that has been isolated at the',\n",
              "  'Deep Learning: Basics and CNN 85\\nTanh Sigmoid ReLU (a) (b) (c) \\nFig. 5 Overview of different non-linear functions (in green) and their first-order derivative (blue). (a) Hyperbolic \\ntangent function (tanh), (b) sigmoid, and (c) rectified linear unit (ReLU) \\n2.3.2 Non-linear \\nFunctions One of the most important components of deep neural networks is \\nthe non-linear functions, also called activation functions. They convert the linear input signal of a node into non-linear outputs \\nto facilitate the learning of high-order polynomials. There are a lot of different non-linear functions in the literature. In this subsec-\\ntion, we will discuss the most classical non-linearities. \\nHyperbolic Tangent \\nFunction (tanh) One of the most standard non-linear functions is the hyperbolic \\ntangent function, aka the tanh function. Tanh is symmetric around \\nthe origin with a range of values varying from-1 to 1. The biggest \\nadvantage of the tanh function is that it produces a zero-centered \\noutput (Fig. 5a), thereby supporting the backpropagation process \\nthat we will cover in the next section. The tanh function is used \\nextensively for the training of multilayer neural networks. Formally, \\nthe tanh function, together with its gradient, is defined as: \\ng = tanhdxTh= ex-e-x \\nex th e-x \\ng \\nx =1-tanh2dxTh :d5Th \\nOne of the downsides of tanh is the saturation of gradients that \\noccurs for large or small inputs. This can slow down the training of \\nthe networks. \\nSigmoid Similar to tanh, the sigmoid is one of the first non-linear functions that were used to compose deep learning architectures. One of the \\nmain advantages is that it has a range of values varying from 0 to \\n1 (Fig. \\n5b) and therefore is especially used for models that aim to \\npredict a probability as an output. Formally, the sigmoid function, \\ntogether with its gradient, is defined as: \\ng =sdxTh= 1 \\n1th e-x \\ng \\nx =sdxThd1-sdxThTh :d6Th',\n",
              "  'Back\\n-\\nPropagation \\n Through Time (BPTT)\\nThe backpropagation algorithm can be adapted to sequential patterns:\\n',\n",
              "  'Variety of Sequential Transductions\\nDue to the solved task, we can distinguish various unfolded network structures for:\\n*\\nSequence \\n classification (e.g. sentiment classification)\\n*\\nIO transduction (conversion, transfer)\\n*\\nSequence \\n generation (e.g. music generation)\\n*\\nSequence transduction (from one to \\n another, e.g. sequence translation)\\n',\n",
              "  'Deep Learning: Basics and CNN 105\\nMax pooling with \\n2x2 filter and stride 2 Input feature map \\nPooled feature map \\nFig. 16 Effect of a pooling operation. Here, a maximum pooling of size 2x2 with a stride of 2 \\nDownsampling Operations (i.e., Pooling Layers) In many \\nCNN architectures, there is an extensive use of downsampling operations that aim to compress the size of the feature maps and \\ndecrease the computational burden. Otherwise referred to as pool-ing layers, these processing operations are aggregating the values of \\ntheir input depending on their design. Some of the most common \\ndownsampling layers are the maximum pooling, average pooling,o r \\nglobal average pooling. In the first two, either the maximum or the \\naverage value is used as a feature for the output across non-overlapping regions of a predefined pooling size. In the case \\nof the global average pooling, the spatial dimensions are all repre-\\nsented with the average value. An example of pooling is provided in \\nFig. \\n16. \\nStrided Convolution The strided convolution refers to the spe-\\ncific case in which, instead of applying the convolution operation \\nfor every location using a step size (or stride, s) of 1, different step \\nsizes can be considered (Fig. 17). Such an operation will produce a \\nconvolution output with much fewer elements. Convolutional \\nblocks with s>1 can be found on CNN architectures as a way to \\ndecrease the feature sizes in intermediate layers. \\nAtrous or D ilated Convolution Dilated, also called atrous, con-\\nvolution is the convolution with kernels that have been dilated by \\ninserting zero holes (a ` trous in French) between the non-zero \\nvalues of a kernel. In this case, an additional parameter (d) of the \\nconvolution operation is added, and it is changing the distance \\nbetween the kernel elements. In essence, it is increasing the reach of the kernel but keeping the number of trainable parameters the \\nsame. For example, a dilated convolution with a kernel size of 3x3 \\nand a dilation rate of d=2 would be sparsely arranged on a \\n5x5 grid.',\n",
              "  'Deep Learning: Basics and CNN 81\\n2.2 Multilayer \\nPerceptrons The limitation of perceptrons to linear problems can be overcome \\nby using multilayer perceptions, often denoted as MLP. An MLP \\nconsists of at least three layers of neurons: the input layer, a hidden \\nlayer, and an output layer. Except for the input neurons, each \\nneuron uses a non-linear activation function, making it capable of \\ndistinguishing data that is not linearly separable. These layers can \\nalso be called fully connected layers since they connect all the \\nneurons of the previous and of the current layer. It is absolutely \\ncrucial to keep in mind that non-linear functions are necessary for \\nthe network to find non-linear separations in the data (otherwise, \\nall the layers could simply be collapsed together into a single \\ngigantic linear function). \\n2.2.1 A Simple Multilayer \\nNetwork Without loss of generality, an MLP with one hidden layer can be \\ndefined as: \\nzdxTh=gdW1 xTh \\n^y =fdx;W1 ,W2Th=W2 zdxTh ,d2Th \\nwhere gdxTh :  - denotes the non-linear function (which can be \\napplied element-wise to a vector), W1 the matrix of coefficients of \\nthe first layer, and W2 the matrix of coefficients of the second layer. \\nEquivalently, one can write: \\nyc = d1 \\nj =1 W2 \\ndc,jThgdW1 djThxTh, d3Th \\nwhere d 1 is the number of neurons for the hidden layer which \\ndefines the width of the network, W1 \\ndjTh denotes the first column \\nof the matrix W1 , and W2 dc,jTh denotes the c, j element of the matrix \\nW2 . Graphically, a two-layer perceptron is presented in Fig. 2 on\\nx1 \\nx2 \\nxp z3 z2 z1 \\nzd1 1 \\nW1W2 ^y \\n2 ^y \\nFig. 2 An example of a simple multilayer perceptron model. The input layer is fed \\ninto a hidden layer (z), which is then combined for the last output layer providing \\nthe final prediction',\n",
              "  'Encoding\\n Networks\\nFor a given sequence s, the encoding network associated to s is formed by \\nunrolling (time unfolding) the recursive network through the input sequence s:\\nIn linear dynamical \\nsystems we can define:\\n',\n",
              "  'birthdays:\\n me: 6th september\\n mother: 25th august\\n father: 9th june\\n grandpa: 2nd july\\n grandma: 3rd october',\n",
              "  'network width, depth, and resolution in a principled way: depth = \\naph , width = bph , resolution =gph s.t. a/C1b2/C1g2 2, a>=1, b>=1, g>=1. \\nIn this formulation, the parameters a, b, g are constants, and a small \\ngrid search can determine them. This grid search resulted in eight \\ndifferent architectures presented in the original paper. EfficientNet \\nis used more and more for medical imaging tasks, as can be seen in \\nmultiple recent studies [ 67-69]. 110 Maria Vakalopoulou et al.\\n5 Autoencoders \\nAn autoencoder is a type of neural network that can learn a com-\\npressed representation (called the latent space representation) of \\nthe training data. As opposed to the multilayer perceptrons and \\nCNNs seen until now that are used for supervised learning, auto-\\nencoders have widely been used for unsupervised learning, with a \\nwide range of applications. The architecture of autoencoders is \\ncomposed of a contracting path (called the encoder), which will transform the input into a lower-dimensional representation, and \\nan expanding path (called the decoder), which will aim at recon-\\nstructing the input as well as possible from the lower-dimensional \\nrepresentation (see Fig. \\n20). \\nThe loss is usually the l 2 loss and the cost function is then: \\nJdth,phTh= n \\ni =1 jj xdiTh-D thdEphdxdiThThThk2 \\n2, d29Th \\nwhere E ph is the encoder (and ph its parameters) and D th is the \\ndecoder (and th its parameters). Note that, in Fig. 20, Dth(Eph(x)) is \\ndenoted as ^x. More generally, one can write: \\nJdth,phTh= x/C24mxref d x,D thdEphdxThTh ,d30Th \\nwhere m ref is the reference distribution that one is trying to approx-\\nimate and d is the reconstruction function. When m ref is the\\nFig. 20 The general principle of a denoising autoencoder. It aims at learning of a \\nlow-dimensional representation (latent space) z of the training data. The learning is done by aiming to provide a faithful reconstruction ^x of the input \\ndata ^x',\n",
              "  'discrete signals f[k] and g[k], with k, the convolution operation \\nis defined by:100 Maria Vakalopoulou et al.\\n0 1 1 1 0 0 0 \\n0 0 1 1 1 0 0 \\n0 0 0 1 1 1 0 \\n0 0 0 1 1 0 0 \\n0 0 1 1 0 0 0 \\n0 1 1 0 0 0 0 \\n1 1 0 0 0 0 0 \\nI * 1 0 1 \\n0 1 0 \\n1 0 1 \\nK = 1 4 3 4 1 \\n1 2 4 3 3 \\n1 2 3 4 1 \\n1 3 3 1 1 \\n3 3 1 1 0 \\nI*K 1 0 1 \\n0 1 0 \\n1 0 1 x1 x0 x1 \\nx0 x1 x0 x1 x0 x1 \\nFig. 10 A visualization of the discrete convolution operation in 2D \\nh 1/2k/C138= \\nn f 1/2k-n/C138g 1/2n/C138:d27Th \\nLastly, the convolution operation can be extended for multidi-\\nmensional signals similarly. For example, we can write the convolu-\\ntion operation between two discrete and finite two-dimensional \\nsignals (e.g., I[i, j], K[i, j]) as: \\nH 1/2i,j/C138= \\nm n I 1/2i-m,j-n/C138K 1/2m,n/C138: d28Th \\nVery often, the first signal will be the input of interest (e.g., a \\nlarge size image), while the second signal will be of relatively small \\nsize (e.g., a 3x3o r4x4 matrix) and will implement a specific \\noperation. The second signal is then called a kernel. In Fig. 10,a \\nvisualization of the convolution operation is shown in the case of a \\n2D discrete signal such as an image and a 3x3 kernel. In detail, the convolution kernel is shifted over all locations of the input, and an \\nelement-wise multiplication and a summation are utilized to calcu-late the convolution output at the corresponding location. Exam-\\nples of applications of convolutions to an image are provided in \\nFig. \\n11. Finally, note that, as in multilayer perceptrons, a convolu-\\ntion will generally be followed by a non-linear activation function, for instance, a ReLU (see Fig. \\n12 for an example of activation \\napplied to a feature map). \\nIn the following sections of this chapter, any reference to the \\nconvolution operation will mostly refer to the 2D discrete case. The',\n",
              "  'to the deep fully connected networks that have been already dis-\\ncussed, CNNs excel in processing data with a spatial or grid-like \\norganization (e.g., time series, images, videos, etc.) while at the \\nsame time decreasing the number of trainable parameters due to \\ntheir weight sharing properties. The rest of this section is first \\nintroducing the convolution operation and the motivation behind \\nusing it as a building block/module of neural networks. Then, a \\nnumber of differen t variations are presented together with exam-\\nples of the most important CNN architectures. Lastly , the impor-\\ntance of the receptive field - a central property of such networks - \\nwill be discussed. Deep Learning: Basics and CNN 99\\n4.1 The Convolution \\nOperation The convolution operation is defined as the integral of the product \\nof the two functions ( f, g)5 after one is reversed and shifted over the \\nother function. Formally, we write: \\nhdtTh= 1\\n-1 fdt-tThgdtTh dt: d25Th \\nSuch an operation can also be denoted with an asterisk (/C3), so it \\nis written as: \\nhdtTh=df/C3gThdtTh:d26Th \\nIn essence, the convolution operation shows how one function \\naffects the other. This intuition arises from the signal processing \\ndomain, where it is typically important to know how a signal will be \\naffected by a filter. For example, consider a uni-dimensional con-\\ntinuous signal, like the brain activity of a patient on some electro-encephalography electrode, and a Gaussian filter. The result of the convolution operation between these two functions will output the \\neffect of a Gaussian filter on this signal which will, in fact, be a \\nsmoothed version of the input. \\nA different way to think of the convolution operation is that it \\nshows how the two functions are related. In other words, it shows \\nhow similar or dissimilar the two functions are at different relative \\npositions. In fact, the convolution operation is very similar to the \\ncross-correlation operation, with the subtle difference being that in the convolution operation, one of the two functions is inverted. In \\nthe context of deep learning specifically, the exact differences \\nbetween the two operations can be of secondary concern; however, \\nthe convolution operation has more properties than correlation, \\nsuch as commutativity. Note also that when the signals are symmet-\\nric, both operations will yield the same result. \\nIn order t o deal with discrete and finite signals, we can expand \\nthe definition of the convolution operation. Specifically, given two\\n5 Note that f and g have no relationship to their previous definitions in the chapter. In particular, f is not the deep \\nlearning model.',\n",
              "  'Long Short\\n -\\nTerm Memory (Vanilla LSTM)\\nExploits a linear memory cell (state) that integrates input information through time\\n :\\n*\\nmemory obtained by self\\n -\\nloop\\n ,\\n*\\ngradient not down\\n -\\nsized by Jacobian of sigmoidal function \\n \\nno vanishing gradient\\n !\\n3 gate units with sigmoid soft\\n -\\nswitch control the information flow via\\n multiplicative \\nconnections\\n :\\n*\\ninput gate \"on\": \\n let input to flow in the memory \\n cell\\n,\\n*\\noutput gate \"on\": \\n let the current value stored in the memory cell to be read in output\\n ,\\n*\\nforget gate \"off\": \\n let the current value stored in the memory cell to be reset to 0\\n .\\n',\n",
              "  'are not covered in the present chapter and are presented, together \\nwith other generative models, in Chap. 5. 112 Maria Vakalopoulou et al.\\n6 Conclusion \\nDeep learning is a very fast evolving field, with numerous still \\nunanswered theoretical questions. However, deep learning-based \\nmodels have become the state-of-the-art methods for a variety of \\nfields and tasks. In this chapter, we presented the basic principles of deep learning, covering both perceptrons and convolutional neural \\nnetworks. All architectures were feedforward and recurrent net-\\nworks are covered in Chap. \\n4. Generative adversarial networks are \\ncovered in Chap. 5, along with other generative models. Chapter 6 \\npresents a recent class of deep learning methods, which does not \\nuse convolutions, and that are called transformers. Finally, through-\\nout the other chapters of the book, different deep learning archi-\\ntectures are presented for various types of applications. \\nAcknowledgements \\nThis work was supported in part by the French government under \\nmanagement of Agence Nationale de la Recherche as part of the \\n\"Investissements d\\'avenir\" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute), reference ANR-10-IAIHU-06 \\n(Institut Hospitalo-Universitaire ICM), and ANR-21-CE45-0007 \\n(Hagnodice). \\nReferences \\n1. Rosenblatt F (1957) The perceptron, a perceiv-\\ning and recognizing automaton Project Para. \\nCornell Aeronautical Laboratory, Buffalo \\n2. Minsky M, Papert S (1969) Perceptron: an \\nintroduction to computational geometry. \\nMIT Press, Cambridge, MA \\n3. Minsky ML, Papert SA (1988) Perceptrons: \\nexpanded edition. MIT Press, Cambridge, MA \\n4. Linnainmaa S (1976) Taylor expansion of the \\naccumulated rounding error. BIT Numer Math \\n16(2):146-160 \\n5. Werbos PJ (1982) Applications of advances in \\nnonlinear sensitivity analysis. In: System mod-eling and optimization. Springer, Berlin, pp 762-770 \\n6. Rumelhart DE, Hinton GE, Williams RJ \\n(1986) Learning representations by back-\\npropagating errors. Nature 323(6088): \\n533-536 7. Le Cun Y (1985) Une proce \\'dure d\\'apprentis-\\nsage pour re \\'seau a ` seuil assyme \\'trique. Cogni-\\ntiva 85:599-604 \\n8. Hochreiter S, Schmidhuber J (1997) Long \\nshort-term memory. Neural Comput 9(8): \\n1735-1780 \\n9. Hinton GE, Osindero S, Teh YW (2006) A fast \\nlearning algorithm for deep belief nets. Neural Comput 18(7):1527-1554 \\n10. Hinton GE (2007) Learning multiple layers of representation. Trends Cogn Sci 11(10): \\n428-434 \\n11. Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei \\nL (2009) ImageNet: a large-scale hierarchical \\nimage database. In: 2009 IEEE conference on \\ncomputer vision and pattern recognition. IEEE, pp 248-255 \\n12. Bergstra J, Bastien F, Breuleux O, Lamblin P, \\nPascanu R, Delalleau O, Desjardins G, Warde-\\nFarley D, Goodfellow I, Bergeron A et al',\n",
              "  'Long Short\\n -\\nTerm Memory (LSTM)\\nFinally, the LSTM decides what is going to the output based on our cell state, but \\nwill be a filtered version. First, a sigmoid layer \\n \\ndecides what parts of the cell \\nstate \\n go \\nto the output. Then\\n ,\\nthe cell state is put through \\n tanh\\n (\\nto push the values \\nto be between -1 and 1) and multiply it by the output of the sigmoid gate\\n ,\\nso that \\nonly the parts are sent to the output.\\nThe \\n output gate (o) \\n of a simple LSTM cell decides what to output from the cell \\nstate which will be done by the sigmoid function \\n \\n.\\nThe input \\n x\\nt\\nis multiplied with \\n tanh\\n to crush the values between (\\n -\\n1,1) and then \\nmultiply it with the output of sigmoid function\\n :\\n',\n",
              "  '4.3 Functions and\\nVariantsnetwork to output similar responses at different locations of the \\ninput. An example of the usefulness of such a property can be \\nidentified on an image detection task. Specifically, when training a \\nnetwork to detect tumors in an MR image of the brain, the model \\nshould respond similarly regardless of the location where the anom-\\naly can be manifested. 104 Maria Vakalopoulou et al.\\nLastly, another important property of the convolution opera-\\ntion is that it decouples the size of the input with the trainable \\nparameters. For example, in the case of MLPs, the size of the weight \\nmatrix is a function of the dimension of the input. Specifically, a densely connected layer that maps 256 features to 10 outputs \\nwould have a size of W\\n10x256 . On the contrary, in convolu-\\ntional layers, the number of trainable parameters only depends on \\nthe kernel size and the number of kernels that a layer has. This \\neventually allows the processing of arbitrarily sized inputs, for \\nexample, in the case of fully convolutional networks. \\nAn observant reader might have noticed that the convolution \\noperation can change the dimensionality of the produced output. \\nIn the example visualized in Fig. 10, the image of size 7x7, when \\nconvolved with a kernel of size 3x3, produces a feature map of size \\nof 5x5. Even though dimension changes can be avoided with \\nappropriate padding (see Fig. 15 for an illustration of this process) \\nprior to the convolution operation, in some cases, it is actually desired to reduce the dimensions of the input. Such a decrease \\ncan be achieved in a number of ways depending on the task at \\nhand. In this subsection, some of the most typical functions that are utilized in CNNs will be discussed. \\nFig. 15 The padding operation, which involves adding zeros around the image, allows to obtain feature maps \\nthat are of the same size as the original image',\n",
              "  'Vanishing/Exploding Gradient Problems\\nIn both BPTT and RTRL, we come across exploding and vanishing gradient problems:\\nExploding gradients \\n are a problem where large error gradients accumulate and result \\nin very large updates to neural network model weights during training. This effects in \\ninstability of the model and difficulty to learn from training data, especially over long \\ninput sequences of data.\\nIn order to robustly store past information, the dynamics of \\nthe network must exhibit \\n attractors but, \\n in their presence, \\ngradients vanish \\n going backward in time, so no learning with\\ngradient descent is possible!\\nTo reduce the vanishing/exploding gradient problems, we can:\\nModify or change the architecture or the network model:\\n*\\nLong Short\\n -\\nTerm Memory (LSTM) units\\n*\\nReservoir Computing: Echo State Networks and Liquid State Machines\\nModify or change the algorithm:\\n*\\nHessian Free Optimization\\n*\\nSmart Initialization: pre\\n -\\ntraining techniques\\n*\\nClipping gradients (check for and limit the size of gradients during the training of the network)\\n*\\nTruncated Backpropagation through time (updating across fewer prior time steps during training)\\n*\\nWeight Regularization (apply a penalty to the networks loss function for large weight values)\\n',\n",
              "  'Shallow Recurrent Neural Networks\\nA shallow Recurrent Neural Network (RNN) defines a non\\n -\\nlinear dynamical system:\\nwhere the functions \\n f\\nand \\n g\\nare non\\n -\\nlinear functions (e.g. \\n tanh\\n ), \\nand h\\n0\\n= 0 or can be learned jointly with the other parameters.\\n',\n",
              "  'A standard network will not work!\\nWhen dealing with sequential data (like sentences of words):\\n\\nInputs\\n and\\n outputs can usually be different lengths in different examples.\\n\\nThe same words in the different examples do not share the same inputs and\\nfeatures learned across different positions of text.\\nThe standard networks that require to associate inputs with features will not work!\\nWe\\nneed\\n to\\nfind\\n another neural network structure that can work with sequences of inputs \\n(e.g. words) that can move the position in the sequences and take into account the context of \\nprevious inputs (e.g. words).\\n',\n",
              "  'Back\\n-\\nPropagation \\n Through Time (BPTT)\\nThe \\n backpropagation algorithm\\n can be \\n adapted\\n to sequential patterns:\\n',\n",
              "  'Introduction\\n\\nHuman thinking process \\n does not start from scratch every second for each pattern \\nas is usually processed in CNNs and classic artificial neural networks \\n -\\nwhat is their major \\nshortcoming between \\n others.\\n\\nWe always take into account \\n previous words, \\n situations, \\n and states\\n of our brains, not \\nthrowing away all previous thoughts during e.g. speech \\n recognition, machine translation, \\nentity names recognition, sentiment classification, music generation, or \\n image captioning.\\n\\nOur intelligence works so \\n well \\n because it is not started again and again for every new \\nsituation but \\n incorporates the \\n knowledge\\n that is gradually formed \\n in time\\n . Thanks to it, \\nall next intelligent processes take into account our \\n previous experiences\\n .\\n\\nRecurrent neural networks \\n address this issue, implementing various loops, allowing \\ninformation to persist, and gradually processing data in time (following time steps).\\n\\nWe can into account previous state of the network, previous inputs and/or previous outputs \\nduring computations.\\n\\nThis \\n chain\\n -\\nlike nature \\n reveals that recurrent neural networks are intimately related to \\nsequences\\n and \\n lists\\n, and are the natural neural network architecture for such data.\\nt',\n",
              "  'neural networks, different optimizers have integrated them into \\ntheir optimization process in the form of weight decay.94 Maria Vakalopoulou et al.\\nValidation \\nTraining Loss \\nTime (epochs) Underfitting Overfitting \\nFig. 8 Illustration of the concept of early stopping. The model that should be selected corresponds to the \\ndashed bar which is the point where the validation loss starts increasing. Before this point, the model is \\nunderfitting. After, it is overfitting \\nWeight Initialization The way that the weights of neural net-\\nworks will be initialized is very important, and it can determine \\nwhether the algorithm converges at all, with some initial points \\nbeing so unstable that the algorithm encounters numerical difficul-\\nties and fails altogether [ 24]. Most of the time, the weights are \\ninitialized randomly from a Gaussian or uniform distribution. \\nAccording to [ 24], the choice of Gaussian or uniform distribution \\ndoes not seem to matter very much; however, the scale does have a \\nlarge effect both on the outcome of the optimization procedure \\nand on the ability of the network to generalize. Nevertheless, more \\ntailored approaches have been developed over the last decade that have become the standard initialization points. One of them is the \\nXavier Initialization [\\n38] which balances between all the layers to \\nhave the same activation variance and the same gradient variance. \\nMore formally the weights are initialized as: \\nWi,j/C24Uniform-6 \\nm th n , 6 \\nmth n , d20Th \\nwhere m is the number of inputs and n the number of outputs of \\nmatrix W. Moreover, the biases b are initialized to 0. \\nDrop-out There are othe r techniques to prevent overfitting, such \\nas drop-out [ 39], which involves randomly destroying neurons \\nduring the training process, thereby reducing the complexity of',\n",
              "  'Chapter 3 \\nDeep Learning: Basics and Convolutional Neural Networks \\n(CNNs) \\nMaria Vakalopoulou, Stergios Christodoulidis, Ninon Burgos, \\nOlivier Colliot, and Vincent Lepetit \\nAbstract \\nDeep learning belongs to the broader family of machine learning methods and currently provides state-of-\\nthe-art performance in a variety of fields, including medical applications. Deep learning architectures can be \\ncategorized into different groups depending on their components. However, most of them share similar modules and mathematical formulations. In this chapter, the basic concepts of deep learning will be \\npresented to provide a better understanding of these powerful and broadly used algorithms. The analysis \\nis structured around the main components of deep learning architectures, focusing on convolutional neural networks and autoencoders. \\nKey words Perceptrons, Backpropagation, Convolutional neural networks, Deep learning, Medical \\nimaging \\n1 Introduction \\nRecently, deep learning frameworks have become very popular, \\nattracting a lot of attention from the research community. These \\nframeworks provide machine learning schemes without the need \\nfor feature engineering, while at the same time they remain quite flexible. Initially developed for supervised tasks, they are nowadays \\nextended to many other settings. Deep learning, in the strict sense, \\ninvolves the use of multiple layers of artificial neurons. The first \\nartificial neural networks were developed in the late 1950s with the presentation of the perceptron [\\n1] algorithms. However, limita-\\ntions related to the computational costs of these algorithms during \\nthat period, as well as the often-miscited claim of Minsky and \\nPapert [ 2] that perceptrons are not capable of learning non-linear \\nfunctions such as the XOR, caused a significant decline of interest \\nfor further research on these algorithms and contributed to the so-called artificial intelligence winter. In particular, in their book \\n[\\n2], Minsky and Papert discussed that single-layer perceptrons are\\nOlivier Colliot (ed.), Machine Learning for Brain Disorders, Neuromethods, vol. 197, https://doi.org/10.1007/978-1-0716-3195-9_3 , \\n(c) The Author(s) 2023\\n77',\n",
              "  'Additional Architectural Features of RNN\\nWe can \\n create \\n Bidirectional Recurrent Neural Networks (BRNN) \\nfor off\\n -\\nline processing or when the sequences are not temporal \\nto predict not only next but also previous sequence elements:\\n',\n",
              "  'array when the input is a 2D image (and a 4D array when the input \\nis a 3D image). Deep Learning: Basics and CNN 103\\nFig. 13 For a given layer, several (usually many) filters are learned, each of them being able to detect a \\nspecific characteristic in the image, resulting in several feature/filter maps. On the other hand, for a given \\nfilter, the weights are shared across all the locations of the image \\nFig. 14 The different feature maps for a given layer are arranged along another dimension. The feature maps will thus be a 3D array when the input is a 2D image (and a 4D array when the input is a 3D image) \\nConvolutional neural networks have proven quite powerful in \\nprocessing data with spatial structure (e.g., images, videos, etc.). \\nThis is effectively based on the fact that there is a local connectivity \\nof the kernel elements while at the same time the same kernel is \\napplied at different locations of the input. Such processing grants a \\nquite useful property called translation equivariance enabling the',\n",
              "  \"Deep Learning: Basics and CNN 89\\nFig. 6 The gradient descent algorithm. This first-order optimization algorithm is \\nfinding a local minimum by taking steps toward the opposite direction of the gradient \\n3.1 Gradient Descent Gradient descent is an iterative optimization algorithm that is \\namong the most popular and basic algorithms in machine learning. \\nIt is a first-order1 optimization algorithm, which is finding a local \\nminimum of a differentiable function. The main idea of gradient \\ndescent is to take iterative steps toward the opposite direction of the \\ngradient of the function that needs to be optimized (Fig. 6). \\nThat way, the parameters W of the model are updated by: \\nWtth1 -Wt-e JdWtTh \\nWt , d14Th \\nwhere t is the iteration and e, called learning rate, is the hyperpara-\\nmeter that indicates the magnitude of the step that the algorithm \\nwill take. \\nBesides its simplicity, gradient descent is one of the most com-\\nmonly used algorithms. More sophisticated algorithms require computing the Hessian (or an approximation) and/or its inverse (or an approximation). Even if these variations could give better \\noptimization guarantees, they are often more computationally expensive, making gradient descent the default method for \\noptimization. \\nIn the case of convex functions, the optimization problem can \\nbe reduced to the problem of finding a local minimum. Any local \\nminimum is then guaranteed to be a global minimum, and gradient \\ndescent can identify it. However, when dealing with non-convex \\nfunctions, such as neural networks, it is possible to have many local \\nminima making the use of gradient descent challenging. Neural \\nnetworks are, in general, non-identifiable [ 24]. A model is said to \\nbe identifiable if it is theoretically possible, given a sufficiently large \\ntraining set, to rule out all but one set of the model's parameters. \\nModels with latent variables, such as the hidden layers of neural \\nnetworks, are often not identifiable because we can obtain equiva-lent models by exchanging latent variables with each other.\\n1 First-order means here that the first-order derivatives of the cost function are used as opposed to second-order \\nalgorithms that, for instance, use the Hessian.\",\n",
              "  'We often use a simplified notation to compute at\\nand ooyt\\nwhich \\n stacks the weight matrices \\nand also speed up computations a bit because we do not need to operate on two matrices and \\nadding the multiplication results when computing at\\nbut multiplying only once in parallel:\\nat=gaWaaat-1+Waxxt+ba=gaWaat-1,xt+ba\\nga\\nis usually \\n ReLU\\n or tan\\nooyt=gyWyaat+by=gyWyat+by\\ngy\\nis usually sigmoid\\nSimplification of the notation\\n',\n",
              "  'Real\\n-\\nTime Recurrent Learning\\n (RTRL)\\nReal\\n -\\nTime Recurrent Learning (RTRL) adapted to sequential patterns:\\n',\n",
              "  'Gated Recurrent Unit (GRU)\\nThe gated recurrent unit combines the forget and input gates into a single \\nupdate gate and merges the \\n ce\\nl\\nl state and hidden state together with some \\nother minor changes. In result the GRU units are simpler \\n than \\n LSTM ones:\\n',\n",
              "  'Bibliography and Literature\\n1.\\n Ian \\n Goodfellow\\n , \\nYoshua\\n Bengio\\n , Aaron \\n Courville\\n , Deep Learning, MIT Press, 2016, \\nISBN 978\\n -\\n1\\n-\\n59327\\n -\\n741\\n -\\n3 or PWN 2018.\\n2.\\n Holk\\n Cruse,\\n Neural Networks as Cybernetic Systems\\n , 2nd and revised edition\\n3.\\n R. Rojas,\\n Neural Networks\\n , Springer\\n -\\nVerlag\\n , Berlin, 1996.\\n4.\\n Convolutional Neural Network\\n (Stanford)\\n5.\\n Visualizing and Understanding Convolutional Networks\\n , \\nZeiler\\n , Fergus, ECCV 2014\\n6.\\n Lectures of Alessandro \\n Sperduti\\n of \\nUniversita\\n Degli\\n Studi\\n di \\nPadova\\n7.\\n Exploading\\n Gradient Problem\\n8.\\n LSTM \\n cells\\n from \\n scratch\\n and the \\n code\\n9.\\n Understanding LSTM\\nUniversity of Science \\nand Technology\\nin Krakow, Poland\\nAdrian Horzyk\\nhorzyk@agh.edu.pl\\nGoogle: \\n Horzyk\\n',\n",
              "  'Real\\n-\\nTime Recurrent Learning\\n (RTRL)\\nReal\\n -\\nTime Recurrent Learning (RTRL) computes partial derivatives \\nduring the forward phase:\\n',\n",
              "  'Long Short\\n -\\nTerm Memory (LSTM)\\nLong Short\\n -\\nTerm Memory networks are a special kind of Recurrent Neural \\nNetworks, containing four (instead of one) interacting layers and capable of \\nlearning long\\n -\\nterm dependencies.\\n',\n",
              "  'Unification of Various Sequence Tasks\\nWe can easily unify all the presented tasks:\\n',\n",
              "  'Variants of LSTM\\nPeephole connections can be added to some or all the gates of the LSTM cells:\\nThe forget gate can be coupled to forget only when we are going to put \\nsomething in the place\\n of the forgotten older state:\\n',\n",
              "  'Prediction of Sequence Elements\\nWe can try to predict a next word in a \\n sentence, more generally\\n , \\na next \\nelement in a \\n sequence, \\n we usually use a few previous words, e.g.:\\n,,I \\ngrew up in England\\n . Thanks to it, I speak fluent ............\" (English)\\nRNNs are capable of handling such long\\n -\\nterm dependencies.\\n',\n",
              "  'Long Short\\n -\\nTerm Memory (LSTM)\\nA simple LSTM cell consists of four gates:\\n*\\nForget gate (f) \\n -\\nwhether and to what extend to forget (erase) \\n the \\nprevious C\\nt\\n-\\n1\\ncell\\n*\\nInput \\n gate (\\n i\\n) \\n-\\nit controls writing to the \\n cell and how strong the given input influence \\nthe output result and combines it with the previous cell output\\n*\\nOutput gate (o) \\n -\\nhow much to reveal the \\n cell and use for computing the output \\n h\\nt\\n',\n",
              "  'Deep Learning: Basics and CNN 109\\nResNet One of the most commonly used CNN architectures, even \\ntoday, is the ResNet [ 57]. ResNet reduced the error rate on Ima-\\ngeNet to 3.6%, while it was the first deep architecture that proposed \\nnovel concepts on how to gracefully go deeper than a few dozen of \\nlayers. In particular, the authors introduced a deep residual learning \\nframework. The main idea of this residual learning is that instead of \\nlearning the desired underlying mapping of each network level, \\nthey learn the residual mapping. More formally, instead of learning \\nthe H(x) mapping after the convolutional and non-linear layers, \\nthey fit another mapping of F(x)=H(x)-x on which the original mapping is recast into F(x)+x. Feedforward neural networks can \\nrealize this mapping with \"shortcut connections\" by simply \\nperforming identity mapping, and their outputs are added to the \\noutputs of the stacked layers. Such identity connections add neither \\nadditional complexity nor parameters to the network, making such \\narchitectures extremely powerful. \\nDifferent ResNet architectures have been proposed even in the \\noriginal paper. Even though the depth of the network is increased with the additional convolutions, especially for the 152-layer \\nResNet (11.3 billion floating point operations), it still has lower complexity (i.e., fewer parameters) than VGG16/VGG19 net-\\nworks. Currently, different layered-size ResNet architectures \\npre-trained on ImageNet are used as backbones for various pro-\\nblems and applications, including medical imaging. Pre-trained ResNet models, even if they are 2D architectures, are commonly \\nused on histopathology [\\n58, 59], chest X-ray [ 60], or even brain \\nimaging [ 61, 62], while the way that such pre-trained networks \\nwork for medical applications gathered the attention of different \\nstudies such as [ 63]. However, it should be noted that networks \\npre-trained on ImageNet are not always efficient for medical imag-\\ning tasks, and there are cases where they perform poorly, much \\nlower than simpler CNNs trained from scratch [ 64]. Nevertheless, \\na pre-trained ResNet is very often a good idea to use for a first try in \\na given application. Finally, there was an effort from the medical \\ncommunity to train 3D variations of ResNet architectures on a large amount of 3D medical data and release the pre-trained mod-\\nels. Such an effort is presented in [\\n65] in which the authors trained \\nand released different 3D ResNet architectures trained on different \\npublicly available 3D datasets, including different anatomies such as \\nthe brain, prostate, liver, heart, and pancreas. \\nEfficientNet A m ore recent CNN architecture that is worth men-\\ntioning in this section is the recently presented EfficientNet \\n[66]. EfficientNets are a family of neural networks that are balanc-\\ning all dimensions of the network (width/depth/resolution) auto-\\nmatically. In particular, the authors propose a simple yet effective \\ncompound scaling method for obtaining these hyperpameters. In \\nparticular, the main compound coefficient ph uniformly scales',\n",
              "  \"114 Maria Vakalopoulou et al.\\noverfitting. J Mach Learn Res 15(1): \\n1929-1958 \\n40. Deng L (2012) The MNIST database of hand-\\nwritten digit images for machine learning \\nresearch. IEEE Signal Process Mag 29(6): \\n141-142 \\n41. Pe'rez-Garci 'a F, Sparks R, Ourselin S (2021) \\nTorchIO: a Python library for efficient loading, \\npreprocessing, augmentation and patch-based \\nsampling of medical images in deep lear ning. \\nComput Methods Programs Biomed 208: \\n106236 \\n42. Ioffe S, Szegedy C (2015) Batch normaliza-\\ntion: accelerating deep network training by reducing inter nal covariate shift. In: Interna-\\ntional conference on machine learning, \\nPMLR, pp 448-456 \\n43. Brock A, De S, Smith SL, Simonyan K (2021) \\nHigh-performance large-scale image recogni-\\ntion without normalization. In: International \\nconference on machine learning, PMLR, pp \\n1059-1071 \\n44. Ruder S (2016) An overview of gradient \\ndescent optimization algorithms. arXiv pre-\\nprint arXiv:160904747 \\n45. Polyak BT (1964) Some methods of speeding up the convergence of iteration methods. USSR Comput Math Math Phys 4(5):1-17 \\n46. Qian N (1999) On the momentum term in \\ngradient descent learning algorithms. Neural \\nNetw 12(1):145-151 \\n47. Duchi J, Hazan E, Singer Y (2011) Adaptive \\nsubgradient methods for online learning and \\nstochastic optimization. J Mach Learn Res 12(7) \\n48. Kingma DP, Ba J (2014) Adam: a method for \\nstochastic optimization. arXiv preprint \\narXiv:14126980 \\n49. Liu L, Jiang H, He P, Chen W, Liu X, Gao J, \\nHan J (2019) On the variance of the adaptive \\nlearning rate and beyond. arXiv preprint arXiv:190803265 \\n50. Zhang M, Lucas J, Ba J, Hinton GE (2019) \\nLookAhead optimizer: k steps forward, 1 step \\nback. Adv Neural Inf Process Syst 32 \\n51. Fukushima K, Miyake S (1982) Neocognitron: \\na self-organizing neural network model for a \\nmechanism of visual pattern recognition. In: \\nCompetition and cooperation in neural nets. \\nSpringer, Berlin, pp 267-285 \\n52. Araujo A, Norris W, Sim J (2019) Computing \\nreceptive fields of convolutional neural net-\\nworks. Distill https:/ /doi.org/10.23915/dis \\ntill.00021 \\n53. LeCun Y, Boser B, Denker JS, Henderson D, \\nHoward RE, Hubbard W, Jackel LD (1989) \\nBackpropagation applied to handwritten zip code recognition. Neural Comput 1(4): \\n541-551 \\n54. Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet classification with deep convolu-\\ntional neural networks. In: Pereira F, Burges C, Bottou L, Weinberger K (eds) Advances in neural information processing sys-\\ntems, vol 25. Curran Associates. \\nhttps:/ / \\nproceedings.neurips.cc/paper/2012/file/c3 \\n99862d3b9d6b76c8436e924a68c45b-\\nPaper.pdf \\n55. Simonyan K, Zisserman A (2014) Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:14091556 \\n56. Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabino-\\nvich A (2015) Going deeper with \\nconvolutions. In: Proceedings of the IEEE \\nconference on computer vision and pattern \\nrecognition, pp 1-9 \\n57. He K, Zhang X, Ren S, Sun J (2016) Deep \\nresidual learning for image recognition. In: Proceedings of the IEEE conference on com-\\nputer vision and pattern recognition, pp \\n770-778 \\n58. Lu MY, Williamson DF, Chen TY, Chen RJ, Barbieri M, Mahmood F (2021) Data-efficient and weakly supervised computational pathol-\\nogy on whole-slide images. Nat Biomed Eng \\n5(6):555-570 \\n59. Benkirane H, Vakalopoulou M, Christodoulidis S, Garberis IJ, Michiels S, \\nCour ne`de PH (2022) Hyper-AdaC: adaptive \\nclustering-based hypergraph representation of whole slide images for survival analysis. In: Machine learning for health, PMLR, pp \\n405-418 \\n60. Horry MJ, Chakraborty S, Paul M, Ulhaq A, Pradhan B, Saha M, Shukla N (2020) X-ray \\nimage based COVID-19 detection using \\npre-trained deep learning models. Engineering \\nArchive, Menomonie \\n61. Li JP, Khan S, Alshara MA, Alotaibi RM, Mawuli C et al (2022) DACBT: deep learning \\napproach for classification of brain tumors \\nusing MRI data in IoT healthcare environ-\\nment. Sci Rep 12(1):1-14 \\n62. Nandhini I, Manjula D, Sugumaran V (2022) \\nMulti-class brain disease classification using \\nmodified pre-trained convolutional neural net-\\nworks model with substantial data augmenta-tion. J Med Imaging Health Inform 12(2): \\n168-183 \\n63. Raghu M, Zhang C, Kleinberg J, Bengio S \\n(2019) Transfusion: understanding transfer \\nlearning for medical imaging. In: Advances in neural information processing systems, vol 32\",\n",
              "  'Unfolding Time and\\nNext Sequence Elements\\nThe sequence can be modeled by a deep \\nfeedforward neural network which weights \\ncan be computed using backpropagation:\\n*\\nh\\nt\\n-\\nis the last state of the whole sequence,\\n*\\nw\\n-\\nweights are shared between layers \\n(are replicated, the same\\n ).\\n',\n",
              "  'Sequential Data and Domains\\nSequential patterns \\n differ from static patterns because:\\n*\\nsuccessive data (points) are strongly correlated,\\n*\\nthe succession of data is crucial from their recognition/classification point of view.\\nA sequence \\n can be defined using mathematical induction as an external vertex or \\nan ordered pair (\\n t,h\\n) where the head h is a vertex and the tail t is a sequence:\\n',\n",
              "  'Back\\n-\\nPropagation \\n Through Time (BPTT)\\nThe \\n backpropagation algorithm\\n can be \\n adapted\\n to sequential patterns:\\n',\n",
              "  'Long Short\\n -\\nTerm Memory (LSTM)\\nLSTM is an extension of RNN that can deal with long\\n -\\nterm temporal \\ndependencies. It implements a mechanism that allows the networks\\nto \"remember\" relevant information for a long period of time:\\n',\n",
              "  \"Deep Learning: Basics and CNN 97\\ngt -GdWtTh \\nrt -rt-1 th gt gt \\nDWt --e \\ndth rtp gt \\nWtth1 -Wt thDWt ,d22Th \\nwhere gt is the gradient estimate vector in time-step t, rt is the term \\ncontrolling the per parameter update, and d is some small quantity \\nthat is used to avoid the division by zero. Note that rt constitutes of \\nthe gradient's element-wise product with itself and of the previous \\nterm rt-1 accumulating the gradients of the previous terms. \\nThis algorithm performs very well for sparse data since it \\ndecreases the learning rate faster for the parameters that are more frequent and slower for the infrequent parameters. However, since \\nthe update accumulates gradients of the previous steps, the updates \\ncould decrease very fast, blocking the learning process. This limita-\\ntion is mitigated by extensions of the AdaGrad algorithm as we \\ndiscuss in the next sections. \\n3.4.3 RMSProp Another algorithm with adaptive learning rates per parameter is the \\nroot mean squared propagation (RMSProp) algorithm, proposed \\nby Geoffrey Hinton. Despite its popularity and use, this algorithm \\nhas not been published. RMSProp is an extension of the AdaGrad \\nalgorithm dealing with the problem of radically diminishing learning rates by being less influenced by the first iterations of the \\nalgorithm. More formally: \\ngt -GdWtTh \\nrt -rrt-1 thd1-rThgt gt \\nDWt --e \\ndth rtp gt \\nWtth1 -Wt thDWt ,d23Th \\nwhere r is a hyperparameter that controls the contribution of the previous gradients and the current gradient in the current update. \\nNote that RMSProp estimates the squared gradients in the same \\nway as AdaGrad, but instead of letting that estimate continually accumulate over training, we keep a moving average of it, integrat-\\ning the momentum. Empirically, RMSProp has been shown to be \\nan effective and practical optimization algorithm for deep neural \\nnetworks [\\n24]. \\n3.4.4 Adam The effectiveness and advantages of the AdaGrad and RMSProp \\nalgorithms are combined in the adaptive moment estimation \\n(Adam) optimizer [ 48]. The method computes individual adaptive \\nlearning rates for different parameters from estimates of the first \\nand second moments of the gradients. More formally:\",\n",
              "  'Deep Dilated Recurrent Neural Networks\\n',\n",
              "  'Back\\n-\\nPropagation \\n Through Time (BPTT)\\nThe \\n backpropagation algorithm\\n can be \\n adapted\\n to sequential patterns:\\n',\n",
              "  'Examples of Sequential Data\\nExamples of sequential data where context is defined by sequences of data:\\nECG signals:\\n Genes and Chromosomes:\\nSpeech signals (sequences of letters, words, phonemes, or audio time data):\\n'],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'included': ['metadatas', 'documents']}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Quering**"
      ],
      "metadata": {
        "id": "UQ8033e4Qj0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "START_MSG = \"\"\"You are a helpful assistant. Your task is to answer questions based only on the context provided.\n",
        "If certain parts of the context are not relevant to the question, you can ignore them.\n",
        "If you cannot find the answer within the context, please respond by saying that you can't find the answer in the given context.\n",
        "Always aim to provide clear and understandable answers. When answering please tell on what page you found information and from which document.\n",
        "\"\"\"\n",
        "API_KEY = ''"
      ],
      "metadata": {
        "id": "sx-YC769VY6T"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(kw_only=True)  # since python 3.10 kw_only=True\n",
        "class TurboRAG(VectorDatabase):\n",
        "   # openai_api_key: Optional[str] = None  # for older python\n",
        "    openai_api_key: int\n",
        "    model: str = \"gpt-4o-mini\"\n",
        "    def __post_init__(self) -> None:\n",
        "        super().__post_init__()\n",
        "\n",
        "        self.chatgpt_client = OpenAI(api_key=self.openai_api_key)\n",
        "        self.vector_db = self.get_db()\n",
        "\n",
        "    def get_context(self, query: str, k: int = 5) -> str:\n",
        "        context = \"\"\n",
        "        raw_context = self.query_db(query=query, k=k)\n",
        "\n",
        "        for res in raw_context:\n",
        "            page = res.metadata.get(\"page\", None)\n",
        "            page = page + 1 if page else None\n",
        "\n",
        "            context += f\"PAGE: {page} DOCUMENT: {res.metadata['source'].split('/')[-1]} - {res.page_content}\\n\"\n",
        "        return context\n",
        "\n",
        "    def prepare_prompt(self, query: str, context: str, start_msg: str = START_MSG) -> str:\n",
        "        return f\"{start_msg} \\nQuestion: {query} \\nContext: {context}\"\n",
        "\n",
        "    def get_answer_from_context(self, prompt: str) -> str:\n",
        "        chat_completion = self.chatgpt_client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=self.model)\n",
        "\n",
        "        return chat_completion.choices[0].message.content\n",
        "\n",
        "    def get_answer(self, question: str, start_msg: str = START_MSG) -> str:\n",
        "        context = self.get_context(query=question, k=K)\n",
        "        prompt = self.prepare_prompt(query=question, context=context, start_msg=start_msg)\n",
        "        answer = self.get_answer_from_context(prompt=prompt)\n",
        "\n",
        "        return answer, context, prompt\n",
        "\n",
        "    def chat(self) -> None:\n",
        "        messages = []\n",
        "        messages.append({\n",
        "              \"role\": \"user\",\n",
        "              \"content\": START_MSG\n",
        "        })\n",
        "        chat_completion = self.chatgpt_client.chat.completions.create(\n",
        "                messages=messages,\n",
        "                model=self.model\n",
        "        )\n",
        "        assistant_response = chat_completion.choices[0].message.content\n",
        "        messages.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": assistant_response\n",
        "        })\n",
        "\n",
        "        print(f\"Assistant: {assistant_response}\")\n",
        "\n",
        "        while True:\n",
        "            user_input = input(\"You: \")\n",
        "            if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "              print(\"Ending the chat. Goodbye!\")\n",
        "              break\n",
        "\n",
        "            user_context = self.get_context(query=user_input, k=K)\n",
        "            user_prompt = self.prepare_prompt(query=user_input, context=user_context, start_msg=\"\")\n",
        "\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_prompt\n",
        "            })\n",
        "\n",
        "            chat_completion = self.chatgpt_client.chat.completions.create(\n",
        "                messages=messages,\n",
        "                model=self.model\n",
        "            )\n",
        "\n",
        "            assistant_response = chat_completion.choices[0].message.content\n",
        "            messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": assistant_response\n",
        "            })\n",
        "\n",
        "            print(f\"Assistant: {assistant_response}\")"
      ],
      "metadata": {
        "id": "YozQ6T_AQmrX"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "turbo_rag = TurboRAG(\n",
        "    openai_api_key=API_KEY,\n",
        "    db_path=DB_PATH,\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")"
      ],
      "metadata": {
        "id": "VZ2VA1jHSTyV"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is cnn?\"\n",
        "answer, context, prompt = turbo_rag.get_answer(question=question)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "1anGQBcwSgBU",
        "outputId": "5d623a53-3a93-4e60-ead4-35550c37501d"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'CNN stands for Convolutional Neural Network. It is a type of deep learning model that excels in processing data that has a spatial or grid-like organization, such as images, videos, or time series. CNNs achieve this by utilizing convolution operations and apply a series of transformations to learn features from the input data while reducing the number of trainable parameters through weight sharing properties. A basic CNN architecture typically involves layers that perform convolution, non-linear activation, pooling operations, and fully connected layers for classification or regression tasks.\\n\\nThis information can be found on PAGE 24 of the document \"CNN.pdf\".'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are requirements for New Egg GPUs?\"\n",
        "answer, context, prompt = turbo_rag.get_answer(question=question)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "g1LIVQrIH821",
        "outputId": "657df666-a0b7-47c6-c060-f5b6d885f6c9"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The requirements for New Egg GPUs as mentioned in the context are:\\n\\n1. **Python 3.9**\\n2. **Packages from requirements.txt file**\\n\\nThis information can be found on page 1 of the document titled \"README.md - New Egg GPUs.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is rnn?\"\n",
        "answer, context, prompt = turbo_rag.get_answer(question=question)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "57qiXgydDRzV",
        "outputId": "3740843d-87c2-4c28-9b56-2c164c80e83b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"An RNN, or Recurrent Neural Network, is defined as a non-linear dynamical system that uses non-linear functions for its operations. In a shallow RNN, the system's output depends on its previous states, which allows it to model time series or sequences. This characteristic distinguishes it from other types of neural networks. \\n\\nThis information can be found on PAGE: 16 of the document CI-RNN-LSTM.pdf.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is Deep Feedforward Networks?\"\n",
        "answer, context, prompt = turbo_rag.get_answer(question=question)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "A9p8iEavDcB1",
        "outputId": "ba5afd0e-383e-460a-a74b-47894736c062"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Deep Feedforward Networks are a class of parametric, non-linear, and hierarchical representation models optimized using stochastic gradient descent. They consist of multiple layers where the output of one layer serves as the input for the next in a hierarchical fashion. The network is \"deep\" because it contains multiple layers, and \"feedforward\" indicates that connections between the nodes do not form cycles, meaning that the data moves in one direction, from input to output.\\n\\nA key feature of these networks is the use of non-linear activation functions that allow them to learn complex patterns. Training involves adjusting the model parameters (weights and biases) based on the data provided, enabling the network to learn from its experiences.\\n\\nThis information is found on page 4 of the document \"CNN.pdf - Deep Learning: Basics and CNN\".'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Give examples of non-linear functions\"\n",
        "answer, context, prompt = turbo_rag.get_answer(question=question)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "qd7QA4htDvK1",
        "outputId": "25770987-2f79-45f3-f9e3-aea5d17614f7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Examples of non-linear functions include:\\n\\n1. Hyperbolic Tangent Function (tanh)\\n2. Sigmoid Function\\n3. Rectified Linear Unit (ReLU)\\n\\nThese functions are significant components of deep neural networks as they help convert linear input signals into non-linear outputs. The information can be found on PAGE: 10 of DOCUMENT: CNN.pdf - Deep Learning: Basics and CNN.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is lstm?\"\n",
        "answer, context, prompt = turbo_rag.get_answer(question=question)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "4n1btXRXGAGc",
        "outputId": "d9605763-5df3-402d-c90a-b1031eace2cd"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LSTM stands for Long Short-Term Memory, which is a type of Recurrent Neural Network (RNN) designed to handle long-term temporal dependencies. It features a mechanism that enables the network to \"remember\" relevant information over extended periods. LSTMs consist of four interacting layers and have the capability to learn long-term dependencies. They utilize a cell state that can be manipulated through what are called gates, which control the flow of information. There are three main gates in an LSTM: the input gate, the output gate, and the forget gate, each serving a specific function in managing the information stored in the cell state.\\n\\nThis information can be found on PAGE 43 and PAGE 36 of the document \"CI-RNN-LSTM.pdf.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"whos is author of CI-RNN-LSTM.pdf?\"\n",
        "answer, context, prompt = turbo_rag.get_answer(question=question)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "X2RsPw4jGTtk",
        "outputId": "5765e5c7-07cf-41bf-f53f-b9ff4059f2f6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The author of the document \"CI-RNN-LSTM.pdf\" is Adrian Horzyk, as mentioned on page 47 of the document.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"explain BPTT\"\n",
        "answer, context, prompt = turbo_rag.get_answer(question=question)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "OazpE9cuGgUs",
        "outputId": "11683997-9166-4205-eda8-1ec4085f6b8c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BPTT, or Backpropagation Through Time, is an adaptation of the traditional backpropagation algorithm that is designed to work with sequential patterns often found in temporal data. It allows for the computation of gradients over sequences by unfolding the recurrent network through time and applying the standard backpropagation technique across these unfolded layers. \\n\\nThis method enables the network to learn dependencies that span across time steps, making it particularly relevant for tasks involving sequences, such as time series forecasting or natural language processing.\\n\\nYou can find this information on pages 22, 23, 24, and 27 of the document titled \"CI-RNN-LSTM.pdf.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"when do i have my birthday?\"\n",
        "answer, context, prompt = turbo_rag.get_answer(question=question)\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PvpYWrP3G5qc",
        "outputId": "49cf94fa-1d3b-44f1-ad4d-757bce57d02b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your birthday is on the 6th of September. This information is found on the section titled \"birthdays\" in the document randomData.txt.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chat**"
      ],
      "metadata": {
        "id": "0STTQh2JPFqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "turbo_rag.chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "IB7wElRJ_Sl4",
        "outputId": "830f52cb-3f2f-4ec1-e14b-39db68b30e05"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: Understood! Please provide the context or document with information that I should refer to in order to answer your questions.\n",
            "You: what is rnn\n",
            "Assistant: An RNN, or Recurrent Neural Network, is defined as a non-linear dynamical system that involves functions, where these functions can include non-linear functions such as tanh. The document also mentions the use of additional architectural features like shortcut connections and higher-order states. \n",
            "\n",
            "This information is found on PAGE 16 of the document titled \"CI-RNN-LSTM.pdf\".\n",
            "You: desribe web app in new gpu's api\n",
            "Assistant: The web app in the new GPU's API is constructed using Dash and is described as a tool for visualizing data. It operates on port 8050 and serves the purpose of displaying the collected and processed information regarding GPUs scraped from the Newegg website. The main functionality of the web app is to provide a user interface for the data that has been scraped, transformed, and stored in a database, enabling users to engage with the data effectively.\n",
            "\n",
            "This information can be found in the README.md document under the section titled \"3.4 Web app.\"\n",
            "You: give me examples of non-linear functions\n",
            "Assistant: Examples of non-linear functions (also known as activation functions) include:\n",
            "\n",
            "1. **Hyperbolic Tangent Function (tanh)**: This function is symmetric around the origin and ranges from -1 to 1. It is commonly used in multilayer neural networks as it produces a zero-centered output.\n",
            "\n",
            "2. **Sigmoid Function**: This function ranges from 0 to 1 and is often used in models that predict probabilities. \n",
            "\n",
            "3. **Rectified Linear Unit (ReLU)**: A popular activation function that outputs the input directly if it is positive; otherwise, it outputs zero. \n",
            "\n",
            "These functions are crucial in enabling deep neural networks to learn non-linear patterns from data.\n",
            "\n",
            "This information can be found on PAGE 10 of the document titled \"CNN.pdf - Deep Learning: Basics and CNN.\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-c0fd9546be79>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mturbo_rag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-102-a008f0ff2827>\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bye\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ending the chat. Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BHupxv8Y_YL1"
      },
      "execution_count": 54,
      "outputs": []
    }
  ]
}